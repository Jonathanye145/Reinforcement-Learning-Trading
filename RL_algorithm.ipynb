{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b353146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Walk-Forward Validation with Per-Stock Adaptive Optimizers.\n",
      "\n",
      "===== FOLD: Training on 2000-2007, Testing on 2008-2009 =====\n",
      "Training Decision Transformer on all stocks...\n",
      "Optimizing traditional strategies for each stock...\n",
      "Completed per-stock optimization for generate_rsi_signals\n",
      "Completed per-stock optimization for generate_ma_cross_signals\n",
      "Backtesting all strategies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Backtesting Tickers: 100%|██████████| 210/210 [02:52<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD: Training on 2002-2009, Testing on 2010-2011 =====\n",
      "Training Decision Transformer on all stocks...\n",
      "Optimizing traditional strategies for each stock...\n",
      "Completed per-stock optimization for generate_rsi_signals\n",
      "Completed per-stock optimization for generate_ma_cross_signals\n",
      "Backtesting all strategies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Backtesting Tickers: 100%|██████████| 210/210 [03:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD: Training on 2004-2011, Testing on 2012-2013 =====\n",
      "Training Decision Transformer on all stocks...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 318\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m states.shape[\u001b[32m0\u001b[39m] < \u001b[32m1\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNot enough data for DT. Skipping fold.\u001b[39m\u001b[33m\"\u001b[39m); \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    317\u001b[39m state_dim = states.shape[\u001b[32m2\u001b[39m]; model = DecisionTransformer(state_dim, \u001b[32m2\u001b[39m, d_model=\u001b[32m128\u001b[39m, n_head=\u001b[32m4\u001b[39m, n_layer=\u001b[32m3\u001b[39m, max_ep_len=\u001b[32m10000\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Decision Transformer on all stocks...\u001b[39m\u001b[33m\"\u001b[39m); model = train(model, states, actions, rtg, timesteps, epochs=\u001b[32m5\u001b[39m)\n\u001b[32m    320\u001b[39m tickers = test_df[\u001b[33m'\u001b[39m\u001b[33mTicker\u001b[39m\u001b[33m'\u001b[39m].unique()\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tickers) == \u001b[32m0\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 167\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, states, actions, rtg, timesteps, epochs, batch_size)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(states), batch_size):\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         s_b, a_b, r_b, t_b = \u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, a[i:i+batch_size].to(device), r[i:i+batch_size].to(device), t[i:i+batch_size].to(device)\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m s_b.shape[\u001b[32m0\u001b[39m] == \u001b[32m0\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    169\u001b[39m         action_preds = model(s_b, a_b, r_b.unsqueeze(-\u001b[32m1\u001b[39m), t_b)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONTROL PANEL & CONFIGURATION\n",
    "# ==============================================================================\n",
    "CONFIG = {\n",
    "    \"USE_WALK_FORWARD_VALIDATION\": True,\n",
    "    \"USE_RISK_MANAGEMENT_OVERLAY\": True,\n",
    "    \"USE_DYNAMIC_TARGET_RETURN\": True,\n",
    "    \"WINDOW_SIZE\": 30,\n",
    "    \"MAX_DRAWDOWN_LIMIT\": 0.20,\n",
    "    \"VOLATILITY_LIMIT_ATR\": 1.5,\n",
    "    \"INITIAL_CASH\": 210000,\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. ADVANCED SIMULATION ENGINE (Unchanged)\n",
    "# ==============================================================================\n",
    "class MarketEnvironment:\n",
    "    def __init__(self, num_steps, num_days_per_year=252.):\n",
    "        self.num_steps = num_steps; self.dt = 1. / num_days_per_year\n",
    "        self.interest_rates = self._simulate_ornstein_uhlenbeck(); self.econ_growth = self._simulate_gbm()\n",
    "    def _simulate_ornstein_uhlenbeck(self):\n",
    "        kappa, theta, sigma, rates = 0.5, 0.02, 0.03, np.zeros(self.num_steps); rates[0] = theta\n",
    "        for t in range(1, self.num_steps): rates[t] = rates[t-1] + kappa * (theta - rates[t-1]) * self.dt + sigma * np.sqrt(self.dt) * np.random.randn()\n",
    "        return rates\n",
    "    def _simulate_gbm(self):\n",
    "        mu, sigma, s0 = 0.05, 0.15, 1.0\n",
    "        return s0 * np.exp((mu - 0.5 * sigma**2) * self.dt + sigma * np.sqrt(self.dt) * np.random.randn(self.num_steps).cumsum())\n",
    "\n",
    "def simulate_heston_path(s0, drift_path, v0, kappa, theta, sigma, rho, num_steps, dt):\n",
    "    prices, variances = np.zeros(num_steps), np.zeros(num_steps); prices[0], variances[0] = s0, v0\n",
    "    for t in range(1, num_steps):\n",
    "        w_s = np.random.randn(); w_v = rho * w_s + np.sqrt(1 - rho**2) * np.random.randn()\n",
    "        variances[t] = np.maximum(0, variances[t-1] + kappa * (theta - variances[t-1]) * dt + sigma * np.sqrt(variances[t-1] * dt) * w_v)\n",
    "        prices[t] = prices[t-1] * np.exp((drift_path[t-1] - 0.5 * variances[t-1]) * dt + np.sqrt(variances[t-1] * dt) * w_s)\n",
    "    return prices\n",
    "\n",
    "def create_multi_stock_dataframe(num_rows=7000):\n",
    "    dt = 1./252; sectors = {\n",
    "        'TECH': {'n_stocks': 30, 'beta_growth': 1.5, 'beta_rates': -0.2, 'add_vol': 0.15},\n",
    "        'BANK': {'n_stocks': 30, 'beta_growth': 0.8, 'beta_rates': 0.9, 'add_vol': 0.20},\n",
    "        'MFG': {'n_stocks': 30, 'beta_growth': 1.1, 'beta_rates': 0.3, 'add_vol': 0.10},\n",
    "        'UTIL': {'n_stocks': 30, 'beta_growth': 0.3, 'beta_rates': -1.2, 'add_vol': 0.05},\n",
    "        'HEALTH': {'n_stocks': 30, 'beta_growth': 0.5, 'beta_rates': -0.5, 'add_vol': 0.08},\n",
    "        'ENERGY': {'n_stocks': 30, 'beta_growth': 0.9, 'beta_rates': 0.1, 'add_vol': 0.25},\n",
    "        'CONSUMER': {'n_stocks': 30, 'beta_growth': 1.3, 'beta_rates': 0.5, 'add_vol': 0.12},}\n",
    "    env = MarketEnvironment(num_rows); all_stocks_df = []; dates = pd.to_datetime(pd.date_range(start='2000-01-01', periods=num_rows))\n",
    "    for sector, params in sectors.items():\n",
    "        for i in range(params['n_stocks']):\n",
    "            ticker = f\"{sector}_{i+1}\"; base_drift = 0.02\n",
    "            dynamic_drift = base_drift + (params['beta_growth'] * env.econ_growth) + (params['beta_rates'] * env.interest_rates)\n",
    "            s0, v0, theta, kappa, sigma, rho = 100 + np.random.uniform(-20, 20), 0.04 + params['add_vol'] * 0.1, 0.05 + params['add_vol'] * 0.2, 3.0, 0.4, -0.7\n",
    "            prices = simulate_heston_path(s0, dynamic_drift, v0, kappa, theta, sigma, rho, num_rows, dt)\n",
    "            stock_df = pd.DataFrame({'Date': dates, 'Ticker': ticker, 'Sector': sector, 'Close': prices})\n",
    "            returns = stock_df['Close'].pct_change().fillna(0); volatility = returns.rolling(window=5).std().bfill() * 0.75\n",
    "            stock_df['Open'] = stock_df['Close'].shift(1).fillna(stock_df['Close']) + np.random.randn(num_rows) * volatility\n",
    "            stock_df['High'] = stock_df[['Open', 'Close']].max(axis=1) + np.random.uniform(0, 2, num_rows) * volatility\n",
    "            stock_df['Low'] = stock_df[['Open', 'Close']].min(axis=1) - np.random.uniform(0, 2, num_rows) * volatility\n",
    "            all_stocks_df.append(stock_df)\n",
    "    full_df = pd.concat(all_stocks_df).reset_index(drop=True)\n",
    "    full_df = full_df.groupby('Ticker', group_keys=False).apply(calculate_indicators).reset_index(drop=True); return full_df.dropna()\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. INDICATOR CALCULATION & FEATURE ENGINEERING (Unchanged)\n",
    "# ==============================================================================\n",
    "def calculate_indicators(df):\n",
    "    df = df.sort_values('Date')\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean(); df['Std_Dev'] = df['Close'].rolling(window=20).std()\n",
    "    df['BB_Upper'] = df['SMA_20'] + (df['Std_Dev'] * 2); df['BB_Lower'] = df['SMA_20'] - (df['Std_Dev'] * 2)\n",
    "    delta = df['Close'].diff(1); gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean(); rs = gain / loss\n",
    "    df['RSI_14'] = 100 - (100 / (1 + rs)); high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['Close'].shift()); low_close = np.abs(df['Low'] - df['Close'].shift()); tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = tr.rolling(window=14).mean(); df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['SMA_200'] = df['Close'].rolling(window=200).mean()\n",
    "    ema_12 = df['Close'].ewm(span=12, adjust=False).mean(); ema_26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = ema_12 - ema_26; df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean(); return df\n",
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler(); self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.feature_cols = ['SMA_20', 'RSI_14', 'ATR_14', 'MACD']\n",
    "        self.scaled_feature_cols = [f\"{c}_scaled\" for c in self.feature_cols]; self.sector_cols = []\n",
    "    def fit_transform(self, data):\n",
    "        data = data.copy(); sector_encoded = self.encoder.fit_transform(data[['Sector']])\n",
    "        self.sector_cols = [f\"is_{cat}\" for cat in self.encoder.categories_[0]]\n",
    "        sector_df = pd.DataFrame(sector_encoded, index=data.index, columns=self.sector_cols)\n",
    "        scaled_features = self.scaler.fit_transform(data[self.feature_cols])\n",
    "        scaled_df = pd.DataFrame(scaled_features, index=data.index, columns=self.scaled_feature_cols)\n",
    "        processed_df = pd.concat([data[['Date', 'Ticker', 'Close', 'ATR_14']], scaled_df, sector_df], axis=1)\n",
    "        processed_df['Norm_ATR'] = data['ATR_14'] / data.groupby('Ticker')['ATR_14'].transform(lambda x: x.rolling(window=252, min_periods=1).mean())\n",
    "        return processed_df.dropna()\n",
    "    def transform(self, data):\n",
    "        data = data.copy(); sector_encoded = self.encoder.transform(data[['Sector']])\n",
    "        sector_df = pd.DataFrame(sector_encoded, index=data.index, columns=self.sector_cols)\n",
    "        scaled_features = self.scaler.transform(data[self.feature_cols])\n",
    "        scaled_df = pd.DataFrame(scaled_features, index=data.index, columns=self.scaled_feature_cols)\n",
    "        processed_df = pd.concat([data[['Date', 'Ticker', 'Close', 'ATR_14']], scaled_df, sector_df], axis=1)\n",
    "        processed_df['Norm_ATR'] = data['ATR_14'] / data.groupby('Ticker')['ATR_14'].transform(lambda x: x.rolling(window=252, min_periods=1).mean())\n",
    "        return processed_df.dropna()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CORE LOGIC (All Unchanged)\n",
    "# ==============================================================================\n",
    "def get_trajectories(data, window_size):\n",
    "    all_states, all_actions, all_rtg, all_timesteps = [], [], [], []\n",
    "    feature_cols = [c for c in data.columns if c.endswith('_scaled') or c.startswith('is_')]\n",
    "    if not feature_cols: return np.array([]), np.array([]), np.array([]), np.array([])\n",
    "    for ticker in data['Ticker'].unique():\n",
    "        ticker_df = data[data['Ticker'] == ticker].copy()\n",
    "        if len(ticker_df) <= window_size: continue\n",
    "        short_ma = ticker_df['Close'].rolling(window=10).mean(); long_ma = ticker_df['Close'].rolling(window=30).mean()\n",
    "        ma_policy_actions = np.where(short_ma > long_ma, 1, 0)\n",
    "        s, a, r, t = _generate_trajectory_from_actions(ticker_df, ma_policy_actions, feature_cols)\n",
    "        if len(s) <= window_size: continue\n",
    "        rewards_to_go = np.cumsum(r[::-1])[::-1]\n",
    "        for i in range(len(s) - window_size):\n",
    "            all_states.append(s[i:i+window_size]); all_actions.append(a[i:i+window_size])\n",
    "            all_rtg.append(rewards_to_go[i:i+window_size]); all_timesteps.append(t[i:i+window_size])\n",
    "    return np.array(all_states), np.array(all_actions), np.array(all_rtg), np.array(all_timesteps)\n",
    "\n",
    "def _generate_trajectory_from_actions(data, actions, feature_cols):\n",
    "    rewards, cash, holdings = [], 10000, 0\n",
    "    for i in range(1, len(data)):\n",
    "        prev_portfolio_val = cash + holdings * data['Close'].iloc[i-1]; action = actions[i]; current_price = data['Close'].iloc[i]\n",
    "        if action == 1 and cash > 0 and current_price > 0: holdings += cash / current_price; cash = 0\n",
    "        elif action == 0 and holdings > 0 and current_price > 0: cash += holdings * current_price; holdings = 0\n",
    "        rewards.append((cash + holdings * current_price) - prev_portfolio_val)\n",
    "    return data[feature_cols].values[1:], actions[1:], np.array(rewards), np.arange(len(actions)-1)\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, d_model, n_head, n_layer, max_ep_len):\n",
    "        super().__init__()\n",
    "        self.state_dim, self.act_dim, self.d_model = state_dim, act_dim, d_model\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, n_head, d_model * 4, 0.1, batch_first=True), n_layer)\n",
    "        self.embed_timestep = nn.Embedding(max_ep_len, d_model); self.embed_return = nn.Linear(1, d_model)\n",
    "        self.embed_state = nn.Linear(state_dim, d_model); self.embed_action = nn.Embedding(act_dim, d_model)\n",
    "        self.embed_ln = nn.LayerNorm(d_model)\n",
    "        self.predict_action = nn.Sequential(nn.Linear(d_model, act_dim), nn.Softmax(dim=-1))\n",
    "    def forward(self, states, actions, returns_to_go, timesteps):\n",
    "        batch_size, seq_len = states.shape[0], states.shape[1]\n",
    "        state_embeds = self.embed_state(states); action_embeds = self.embed_action(actions)\n",
    "        rtg_embeds = self.embed_return(returns_to_go); time_embeds = self.embed_timestep(timesteps)\n",
    "        state_embeds += time_embeds; action_embeds += time_embeds; rtg_embeds += time_embeds\n",
    "        stacked_inputs = torch.stack((rtg_embeds, state_embeds, action_embeds), dim=1).permute(0, 2, 1, 3).reshape(batch_size, 3 * seq_len, self.d_model)\n",
    "        stacked_inputs = self.embed_ln(stacked_inputs); mask = nn.Transformer.generate_square_subsequent_mask(3 * seq_len).to(states.device)\n",
    "        transformer_out = self.transformer(stacked_inputs, mask=mask); state_out = transformer_out[:, 1::3]\n",
    "        return self.predict_action(state_out)\n",
    "\n",
    "def train(model, states, actions, rtg, timesteps, epochs=5, batch_size=256):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); model.to(device).train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    s, a, r, t = torch.from_numpy(states).float(), torch.from_numpy(actions).long(), torch.from_numpy(rtg).float(), torch.from_numpy(timesteps).long()\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(states), batch_size):\n",
    "            s_b, a_b, r_b, t_b = s[i:i+batch_size].to(device), a[i:i+batch_size].to(device), r[i:i+batch_size].to(device), t[i:i+batch_size].to(device)\n",
    "            if s_b.shape[0] == 0: continue\n",
    "            action_preds = model(s_b, a_b, r_b.unsqueeze(-1), t_b)\n",
    "            loss = F.cross_entropy(action_preds.reshape(-1, model.act_dim), a_b.reshape(-1))\n",
    "            optimizer.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n",
    "    return model\n",
    "\n",
    "def backtest_dt(model, data, window_size, initial_cash):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); model.eval().to(device)\n",
    "    cash, holdings, portfolio_values, high_water_mark = initial_cash, 0, [initial_cash], initial_cash\n",
    "    feature_cols = [c for c in data.columns if c.endswith('_scaled') or c.startswith('is_')]\n",
    "    context_states = torch.zeros(1, window_size, model.state_dim, device=device, dtype=torch.float32)\n",
    "    context_actions = torch.zeros(1, window_size, dtype=torch.long, device=device)\n",
    "    context_rtg = torch.zeros(1, window_size, 1, device=device, dtype=torch.float32)\n",
    "    context_timesteps = torch.arange(window_size, device=device).reshape(1, window_size)\n",
    "    for i in range(len(data)):\n",
    "        current_state = torch.from_numpy(data[feature_cols].iloc[i].values).float().reshape(1, 1, model.state_dim).to(device)\n",
    "        context_states = torch.cat([context_states[:, 1:], current_state], dim=1); target_return = initial_cash * 0.1\n",
    "        if CONFIG[\"USE_DYNAMIC_TARGET_RETURN\"] and 'Norm_ATR' in data.columns and not pd.isna(data['Norm_ATR'].iloc[i]) and data['Norm_ATR'].iloc[i] > 0: target_return *= data['Norm_ATR'].iloc[i]\n",
    "        rtg_update = torch.tensor([[[target_return]]], device=device, dtype=torch.float32); context_rtg = torch.cat([context_rtg[:, 1:], rtg_update], dim=1)\n",
    "        with torch.no_grad(): action_preds = model(context_states, context_actions, context_rtg, context_timesteps)\n",
    "        final_action = torch.argmax(action_preds[0, -1, :]).item()\n",
    "        if CONFIG[\"USE_RISK_MANAGEMENT_OVERLAY\"]:\n",
    "             current_drawdown = (high_water_mark - portfolio_values[-1]) / high_water_mark if high_water_mark > 0 else 0\n",
    "             if current_drawdown > CONFIG[\"MAX_DRAWDOWN_LIMIT\"]: final_action = 0\n",
    "             if 'Norm_ATR' in data.columns and not pd.isna(data['Norm_ATR'].iloc[i]) and data['Norm_ATR'].iloc[i] > CONFIG[\"VOLATILITY_LIMIT_ATR\"]: final_action = 0\n",
    "        current_price = data['Close'].iloc[i]\n",
    "        if final_action == 1 and cash > 0 and current_price > 0: holdings += cash / current_price; cash = 0\n",
    "        elif final_action == 0 and holdings > 0 and current_price > 0: cash += holdings * current_price; holdings = 0\n",
    "        new_value = cash + holdings * current_price; portfolio_values.append(new_value); high_water_mark = max(high_water_mark, new_value)\n",
    "        context_actions = torch.cat([context_actions[:, 1:], torch.tensor([[final_action]], device=device)], dim=1)\n",
    "    return pd.Series(portfolio_values[1:], index=data['Date'])\n",
    "\n",
    "def backtest_traditional(data, signals, initial_cash):\n",
    "    cash, holdings, portfolio_values = initial_cash, 0, []\n",
    "    for i in range(len(data)):\n",
    "        signal = signals.iloc[i]; current_price = data['Close'].iloc[i]\n",
    "        if signal == 1 and holdings == 0 and current_price > 0: holdings = cash / current_price; cash = 0\n",
    "        elif signal == -1 and holdings > 0 and current_price > 0: cash = holdings * current_price; holdings = 0\n",
    "        portfolio_values.append(cash + holdings * current_price)\n",
    "    return pd.Series(portfolio_values, index=data.index)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TRADITIONAL STRATEGIES & ADAPTIVE OPTIMIZERS (Unchanged)\n",
    "# ==============================================================================\n",
    "def generate_ma_cross_signals(data, short_window=50, long_window=200):\n",
    "    signals = pd.Series(index=data.index, dtype=int); short_ma = data['Close'].rolling(window=short_window).mean()\n",
    "    long_ma = data['Close'].rolling(window=long_window).mean(); signals[short_ma > long_ma] = 1\n",
    "    signals[short_ma < long_ma] = -1; return signals.ffill().fillna(0)\n",
    "def generate_rsi_signals(data, buy_threshold=30, sell_threshold=70):\n",
    "    signals = pd.Series(index=data.index, dtype=int); signals[data['RSI_14'] < buy_threshold] = 1\n",
    "    signals[data['RSI_14'] > sell_threshold] = -1; return signals.ffill().fillna(0)\n",
    "def generate_macd_signals(data):\n",
    "    signals = pd.Series(index=data.index, dtype=int); signals[data['MACD'] > data['MACD_Signal']] = 1\n",
    "    signals[data['MACD'] < data['MACD_Signal']] = -1; return signals.ffill().fillna(0)\n",
    "class ParameterOptimizer:\n",
    "    def __init__(self, strategy_func, param_grid):\n",
    "        self.strategy_func = strategy_func; self.param_grid = param_grid\n",
    "    def find_best_params_per_stock(self, train_data, tickers):\n",
    "        optimal_params_for_all_stocks = {}\n",
    "        keys, values = zip(*self.param_grid.items())\n",
    "        param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "        if 'short_window' in self.param_grid and 'long_window' in self.param_grid:\n",
    "            param_combinations = [p for p in param_combinations if p['short_window'] < p['long_window']]\n",
    "        for ticker in tickers:\n",
    "            ticker_data = train_data[train_data['Ticker'] == ticker]\n",
    "            if ticker_data.empty: continue\n",
    "            best_ticker_params, best_ticker_sharpe = {}, -np.inf\n",
    "            for params in param_combinations:\n",
    "                signals = self.strategy_func(ticker_data, **params); portfolio = backtest_traditional(ticker_data, signals, 10000)\n",
    "                returns = portfolio.pct_change().dropna(); sharpe_ratio = 0\n",
    "                if not returns.empty and returns.std() > 0: sharpe_ratio = returns.mean() / returns.std()\n",
    "                if sharpe_ratio > best_ticker_sharpe: best_ticker_sharpe, best_ticker_params = sharpe_ratio, params\n",
    "            optimal_params_for_all_stocks[ticker] = best_ticker_params\n",
    "        print(f\"Completed per-stock optimization for {self.strategy_func.__name__}\")\n",
    "        return optimal_params_for_all_stocks\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. --- NEW --- DETAILED PERFORMANCE REPORTING ENGINE\n",
    "# ==============================================================================\n",
    "def calculate_performance_metrics(portfolio_values):\n",
    "    \"\"\"Calculates a dictionary of detailed performance metrics.\"\"\"\n",
    "    if portfolio_values.empty or portfolio_values.iloc[0] == 0: return None\n",
    "    returns = portfolio_values.pct_change().dropna()\n",
    "    if returns.empty or returns.std() == 0: return None\n",
    "\n",
    "    total_return = (portfolio_values.iloc[-1] - portfolio_values.iloc[0]) / portfolio_values.iloc[0]\n",
    "    \n",
    "    num_years = (portfolio_values.index[-1] - portfolio_values.index[0]).days / 365.25\n",
    "    annualized_return = (1 + total_return) ** (1/num_years) - 1 if num_years > 0 else 0\n",
    "    \n",
    "    annualized_volatility = returns.std() * np.sqrt(252)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility if annualized_volatility > 0 else 0\n",
    "    \n",
    "    high_water_mark = portfolio_values.cummax()\n",
    "    drawdown = (portfolio_values - high_water_mark) / high_water_mark\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown < 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Total Return\": total_return,\n",
    "        \"Annualized Return\": annualized_return,\n",
    "        \"Annualized Volatility\": annualized_volatility,\n",
    "        \"Max Drawdown\": max_drawdown,\n",
    "        \"Sharpe Ratio\": sharpe_ratio,\n",
    "        \"Calmar Ratio\": calmar_ratio\n",
    "    }\n",
    "\n",
    "def print_performance_summary(name, metrics):\n",
    "    \"\"\"Prints a formatted summary of performance metrics.\"\"\"\n",
    "    print(f\"\\n--- Performance: {name} ---\")\n",
    "    print(f\"Total Return: {metrics['Total Return']:.2%}\")\n",
    "    print(f\"Annualized Return: {metrics['Annualized Return']:.2%}\")\n",
    "    print(f\"Annualized Volatility: {metrics['Annualized Volatility']:.2%}\")\n",
    "    print(f\"Max Drawdown: {metrics['Max Drawdown']:.2%}\")\n",
    "    print(f\"Sharpe Ratio: {metrics['Sharpe Ratio']:.2f}\")\n",
    "    print(f\"Calmar Ratio: {metrics['Calmar Ratio']:.2f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. MAIN EXECUTION LOGIC (Updated for detailed, per-sector reporting)\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    df = create_multi_stock_dataframe()\n",
    "    if not CONFIG[\"USE_WALK_FORWARD_VALIDATION\"]: pass\n",
    "    else:\n",
    "        print(\"Running Walk-Forward Validation with Per-Stock Adaptive Optimizers.\")\n",
    "        df['Date'] = pd.to_datetime(df['Date']); df = df.set_index('Date'); df.sort_index(inplace=True)\n",
    "        # Create a Ticker -> Sector mapping for later use\n",
    "        ticker_to_sector = df[['Ticker', 'Sector']].drop_duplicates().set_index('Ticker')['Sector'].to_dict()\n",
    "        \n",
    "        unique_years = df.index.year.unique(); train_window_yrs, test_window_yrs = 8, 2\n",
    "        # --- MODIFIED --- Store individual stock results per fold\n",
    "        all_folds_stock_results = []\n",
    "        \n",
    "        rsi_optimizer = ParameterOptimizer(generate_rsi_signals, {'buy_threshold': [25, 30, 35], 'sell_threshold': [70, 75, 80]})\n",
    "        ma_optimizer = ParameterOptimizer(generate_ma_cross_signals, {'short_window': [20, 30, 50], 'long_window': [100, 150, 200]})\n",
    "\n",
    "        for i in range(train_window_yrs, len(unique_years), test_window_yrs):\n",
    "            train_start_year, train_end_year = unique_years[i - train_window_yrs], unique_years[i - 1]\n",
    "            test_start_year = unique_years[i]; test_end_year = unique_years[min(i + test_window_yrs - 1, len(unique_years)-1)]\n",
    "            print(f\"\\n===== FOLD: Training on {train_start_year}-{train_end_year}, Testing on {test_start_year}-{test_end_year} =====\")\n",
    "            train_df, test_df = df.loc[str(train_start_year):str(train_end_year)], df.loc[str(test_start_year):str(test_end_year)]\n",
    "            \n",
    "            engineer = FeatureEngineer(); processed_train = engineer.fit_transform(train_df.reset_index()); \n",
    "            processed_test = engineer.transform(test_df.reset_index())\n",
    "            \n",
    "            states, actions, rtg, timesteps = get_trajectories(processed_train, CONFIG[\"WINDOW_SIZE\"])\n",
    "            if states.shape[0] < 1: print(\"Not enough data for DT. Skipping fold.\"); continue\n",
    "            \n",
    "            state_dim = states.shape[2]; model = DecisionTransformer(state_dim, 2, d_model=128, n_head=4, n_layer=3, max_ep_len=10000)\n",
    "            print(\"Training Decision Transformer on all stocks...\"); model = train(model, states, actions, rtg, timesteps, epochs=5)\n",
    "            \n",
    "            tickers = test_df['Ticker'].unique()\n",
    "            if len(tickers) == 0: continue\n",
    "            cash_per_stock = CONFIG[\"INITIAL_CASH\"] / len(tickers)\n",
    "            \n",
    "            print(\"Optimizing traditional strategies for each stock...\")\n",
    "            best_rsi_params_per_stock = rsi_optimizer.find_best_params_per_stock(train_df, train_df['Ticker'].unique())\n",
    "            best_ma_params_per_stock = ma_optimizer.find_best_params_per_stock(train_df, train_df['Ticker'].unique())\n",
    "\n",
    "            # --- MODIFIED --- Store results for each stock in a dictionary\n",
    "            fold_stock_results = {ticker: {} for ticker in tickers}\n",
    "            print(\"Backtesting all strategies...\")\n",
    "            for t in tqdm(tickers, desc=\"Backtesting Tickers\"):\n",
    "                ticker_data = test_df[test_df['Ticker'] == t]; proc_data = processed_test[processed_test['Ticker'] == t]\n",
    "                if ticker_data.empty: continue\n",
    "                # MA Cross\n",
    "                params = best_ma_params_per_stock.get(t, {})\n",
    "                if params: fold_stock_results[t]['MA Cross (Optimized)'] = backtest_traditional(ticker_data, generate_ma_cross_signals(ticker_data, **params), cash_per_stock)\n",
    "                # RSI\n",
    "                params = best_rsi_params_per_stock.get(t, {})\n",
    "                if params: fold_stock_results[t]['RSI (Optimized)'] = backtest_traditional(ticker_data, generate_rsi_signals(ticker_data, **params), cash_per_stock)\n",
    "                # MACD\n",
    "                fold_stock_results[t]['MACD'] = backtest_traditional(ticker_data, generate_macd_signals(ticker_data), cash_per_stock)\n",
    "                # Benchmark\n",
    "                fold_stock_results[t]['Benchmark'] = ticker_data['Close'] * (cash_per_stock / ticker_data['Close'].iloc[0])\n",
    "                # DT\n",
    "                if not proc_data.empty: fold_stock_results[t]['DT'] = backtest_dt(model, proc_data, CONFIG[\"WINDOW_SIZE\"], cash_per_stock)\n",
    "            \n",
    "            all_folds_stock_results.append(fold_stock_results)\n",
    "\n",
    "        # --- NEW --- FINAL AGGREGATION & DETAILED REPORTING\n",
    "        print(\"\\n\\n\" + \"=\"*60); print(\"===== FINAL WALK-FORWARD PERFORMANCE SUMMARY =====\"); print(\"=\"*60)\n",
    "        \n",
    "        # 1. Combine results from all folds\n",
    "        final_stock_results = {}\n",
    "        for fold_res in all_folds_stock_results:\n",
    "            for ticker, strategies in fold_res.items():\n",
    "                if ticker not in final_stock_results: final_stock_results[ticker] = {}\n",
    "                for strat, series in strategies.items():\n",
    "                    if strat not in final_stock_results[ticker]: final_stock_results[ticker][strat] = []\n",
    "                    final_stock_results[ticker][strat].append(series)\n",
    "        \n",
    "        for ticker, strats in final_stock_results.items():\n",
    "            for strat, series_list in strats.items():\n",
    "                final_stock_results[ticker][strat] = pd.concat(series_list)\n",
    "\n",
    "        # 2. Structure results by strategy and sector\n",
    "        strategy_keys = [\"DT\", \"MA Cross (Optimized)\", \"RSI (Optimized)\", \"MACD\", \"Benchmark\"]\n",
    "        results_by_strategy = {strat: {} for strat in strategy_keys}\n",
    "        for ticker, strats in final_stock_results.items():\n",
    "            sector = ticker_to_sector.get(ticker)\n",
    "            if not sector: continue\n",
    "            for strat, series in strats.items():\n",
    "                if sector not in results_by_strategy[strat]: results_by_strategy[strat][sector] = []\n",
    "                results_by_strategy[strat][sector].append(series)\n",
    "        \n",
    "        # 3. Calculate and print all stats\n",
    "        for strat, sectors in results_by_strategy.items():\n",
    "            strat_name = strat.replace('Benchmark', 'Buy & Hold')\n",
    "            print(\"\\n\" + \"#\"*60); print(f\"## Strategy: {strat_name}\"); print(\"#\"*60)\n",
    "\n",
    "            # --- Overall Performance ---\n",
    "            all_stock_series = [s for sector_stocks in sectors.values() for s in sector_stocks]\n",
    "            if not all_stock_series: continue\n",
    "            overall_portfolio = pd.concat(all_stock_series, axis=1).sum(axis=1)\n",
    "            overall_metrics = calculate_performance_metrics(overall_portfolio)\n",
    "            if overall_metrics:\n",
    "                print_performance_summary(\"Overall Portfolio\", overall_metrics)\n",
    "\n",
    "            # --- Per-Sector Performance ---\n",
    "            print(\"\\n--- Per-Sector Performance Breakdown ---\")\n",
    "            for sector, stock_series_list in sorted(sectors.items()):\n",
    "                sector_metrics = []\n",
    "                for series in stock_series_list:\n",
    "                    metrics = calculate_performance_metrics(series)\n",
    "                    if metrics: sector_metrics.append(metrics)\n",
    "                \n",
    "                if not sector_metrics: continue\n",
    "                \n",
    "                # Aggregate metrics for the sector\n",
    "                avg_metrics = {k: np.mean([m[k] for m in sector_metrics]) for k in sector_metrics[0]}\n",
    "                min_metrics = {k: np.min([m[k] for m in sector_metrics]) for k in sector_metrics[0]}\n",
    "                max_metrics = {k: np.max([m[k] for m in sector_metrics]) for k in sector_metrics[0]}\n",
    "\n",
    "                print(f\"\\n Sector: {sector} ({len(stock_series_list)} stocks)\")\n",
    "                print(f\" {'Metric':<25} | {'Average':>12} | {'Max':>12} | {'Min':>12}\")\n",
    "                print(\"-\" * 65)\n",
    "                for k in avg_metrics:\n",
    "                    is_percent = \"Return\" in k or \"Volatility\" in k or \"Drawdown\" in k\n",
    "                    avg_str = f\"{avg_metrics[k]:.2%}\" if is_percent else f\"{avg_metrics[k]:.2f}\"\n",
    "                    max_str = f\"{max_metrics[k]:.2%}\" if is_percent else f\"{max_metrics[k]:.2f}\"\n",
    "                    min_str = f\"{min_metrics[k]:.2%}\" if is_percent else f\"{min_metrics[k]:.2f}\"\n",
    "                    print(f\" {k:<25} | {avg_str:>12} | {max_str:>12} | {min_str:>12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afaa22b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Main Walk-Forward Validation Process...\n",
      "\n",
      "===== FOLD 4: Training on 2000-2007, Testing on 2008-2009 =====\n",
      "Starting LSTM-PPO training to generate expert trajectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LSTM-PPO per stock:  14%|█▍        | 3/21 [01:37<09:45, 32.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 456\u001b[39m\n\u001b[32m    453\u001b[39m train_df, test_df = df.loc[\u001b[38;5;28mstr\u001b[39m(train_start_year):\u001b[38;5;28mstr\u001b[39m(train_end_year)], df.loc[\u001b[38;5;28mstr\u001b[39m(test_start_year):\u001b[38;5;28mstr\u001b[39m(test_end_year)]\n\u001b[32m    454\u001b[39m engineer = FeatureEngineer(); processed_train = engineer.fit_transform(train_df.reset_index()); processed_test = engineer.transform(test_df.reset_index())\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m ppo_trajectories, trained_ppo_models = \u001b[43mtrain_ppo_and_generate_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m s_ppo, a_ppo, rtg_ppo, t_ppo = get_trajectories(ppo_trajectories, CONFIG[\u001b[33m\"\u001b[39m\u001b[33mWINDOW_SIZE\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s_ppo.shape[\u001b[32m0\u001b[39m] < \u001b[32m1\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNot enough PPO data. Skipping fold.\u001b[39m\u001b[33m\"\u001b[39m); \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 163\u001b[39m, in \u001b[36mtrain_ppo_and_generate_trajectories\u001b[39m\u001b[34m(data, fold_num, writer)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(): action_logits, value = model(state_tensor)\n\u001b[32m    162\u001b[39m dist = Categorical(logits=action_logits); action = dist.sample(); actions.append(action.item())\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m log_prob = \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m; log_probs.append(log_prob); values.append(value)\n\u001b[32m    164\u001b[39m next_state, reward, episode_done, _ = env.step(action.item())\n\u001b[32m    165\u001b[39m rewards.append(reward); episode_rewards += reward; state = next_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/distributions/categorical.py:141\u001b[39m, in \u001b[36mCategorical.log_prob\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._validate_args:\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_sample(value)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.unsqueeze(-\u001b[32m1\u001b[39m)\n\u001b[32m    142\u001b[39m value, log_pmf = torch.broadcast_tensors(value, \u001b[38;5;28mself\u001b[39m.logits)\n\u001b[32m    143\u001b[39m value = value[..., :\u001b[32m1\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import itertools\n",
    "import os\n",
    "import webbrowser\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONTROL PANEL & CONFIGURATION\n",
    "# ==============================================================================\n",
    "CONFIG = {\n",
    "    \"USE_WALK_FORWARD_VALIDATION\": True,\n",
    "    \"USE_RISK_MANAGEMENT_OVERLAY\": True,\n",
    "    \"USE_DYNAMIC_TARGET_RETURN\": True,\n",
    "    \"WINDOW_SIZE\": 30,\n",
    "    \"MAX_DRAWDOWN_LIMIT\": 0.20,\n",
    "    \"VOLATILITY_LIMIT_ATR\": 1.5,\n",
    "    \"INITIAL_CASH\": 210000,\n",
    "}\n",
    "# Define device globally for all models\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. & 2. SIMULATION & FEATURE ENGINEERING (Unchanged)\n",
    "# ==============================================================================\n",
    "class MarketEnvironment:\n",
    "    def __init__(self, num_steps, num_days_per_year=252.):\n",
    "        self.num_steps = num_steps; self.dt = 1. / num_days_per_year\n",
    "        self.interest_rates = self._simulate_ornstein_uhlenbeck(); self.econ_growth = self._simulate_gbm()\n",
    "    def _simulate_ornstein_uhlenbeck(self):\n",
    "        kappa, theta, sigma, rates = 0.5, 0.02, 0.03, np.zeros(self.num_steps); rates[0] = theta\n",
    "        for t in range(1, self.num_steps): rates[t] = rates[t-1] + kappa * (theta - rates[t-1]) * self.dt + sigma * np.sqrt(self.dt) * np.random.randn()\n",
    "        return rates\n",
    "    def _simulate_gbm(self):\n",
    "        mu, sigma, s0 = 0.05, 0.15, 1.0\n",
    "        return s0 * np.exp((mu - 0.5 * sigma**2) * self.dt + sigma * np.sqrt(self.dt) * np.random.randn(self.num_steps).cumsum())\n",
    "\n",
    "def simulate_heston_path(s0, drift_path, v0, kappa, theta, sigma, rho, num_steps, dt):\n",
    "    prices, variances = np.zeros(num_steps), np.zeros(num_steps); prices[0], variances[0] = s0, v0\n",
    "    for t in range(1, num_steps):\n",
    "        w_s = np.random.randn(); w_v = rho * w_s + np.sqrt(1 - rho**2) * np.random.randn()\n",
    "        variances[t] = np.maximum(0, variances[t-1] + kappa * (theta - variances[t-1]) * dt + sigma * np.sqrt(variances[t-1] * dt) * w_v)\n",
    "        prices[t] = prices[t-1] * np.exp((drift_path[t-1] - 0.5 * variances[t-1]) * dt + np.sqrt(variances[t-1] * dt) * w_s)\n",
    "    return prices\n",
    "\n",
    "def create_multi_stock_dataframe(num_rows=7000):\n",
    "    dt = 1./252; sectors = {\n",
    "        'TECH': {'n_stocks': 3, 'beta_growth': 1.5, 'beta_rates': -0.2, 'add_vol': 0.15}, 'BANK': {'n_stocks': 3, 'beta_growth': 0.8, 'beta_rates': 0.9, 'add_vol': 0.20},\n",
    "        'MFG': {'n_stocks': 3, 'beta_growth': 1.1, 'beta_rates': 0.3, 'add_vol': 0.10}, 'UTIL': {'n_stocks': 3, 'beta_growth': 0.3, 'beta_rates': -1.2, 'add_vol': 0.05},\n",
    "        'HEALTH': {'n_stocks': 3, 'beta_growth': 0.5, 'beta_rates': -0.5, 'add_vol': 0.08}, 'ENERGY': {'n_stocks': 3, 'beta_growth': 0.9, 'beta_rates': 0.1, 'add_vol': 0.25},\n",
    "        'CONSUMER': {'n_stocks': 3, 'beta_growth': 1.3, 'beta_rates': 0.5, 'add_vol': 0.12},}\n",
    "    env = MarketEnvironment(num_rows); all_stocks_df = []; dates = pd.to_datetime(pd.date_range(start='2000-01-01', periods=num_rows))\n",
    "    for sector, params in sectors.items():\n",
    "        for i in range(params['n_stocks']):\n",
    "            ticker = f\"{sector}_{i+1}\"; base_drift = 0.02\n",
    "            dynamic_drift = base_drift + (params['beta_growth'] * env.econ_growth) + (params['beta_rates'] * env.interest_rates)\n",
    "            s0, v0, theta, kappa, sigma, rho = 100 + np.random.uniform(-20, 20), 0.04 + params['add_vol'] * 0.1, 0.05 + params['add_vol'] * 0.2, 3.0, 0.4, -0.7\n",
    "            prices = simulate_heston_path(s0, dynamic_drift, v0, kappa, theta, sigma, rho, num_rows, dt)\n",
    "            stock_df = pd.DataFrame({'Date': dates, 'Ticker': ticker, 'Sector': sector, 'Close': prices})\n",
    "            returns = stock_df['Close'].pct_change().fillna(0); volatility = returns.rolling(window=5).std().bfill() * 0.75\n",
    "            stock_df['Open'] = stock_df['Close'].shift(1).fillna(stock_df['Close']) + np.random.randn(num_rows) * volatility\n",
    "            stock_df['High'] = stock_df[['Open', 'Close']].max(axis=1) + np.random.uniform(0, 2, num_rows) * volatility\n",
    "            stock_df['Low'] = stock_df[['Open', 'Close']].min(axis=1) - np.random.uniform(0, 2, num_rows) * volatility\n",
    "            all_stocks_df.append(stock_df)\n",
    "    full_df = pd.concat(all_stocks_df).reset_index(drop=True)\n",
    "    full_df = full_df.groupby('Ticker', group_keys=False).apply(calculate_indicators).reset_index(drop=True); return full_df.dropna()\n",
    "\n",
    "def calculate_indicators(df):\n",
    "    df = df.sort_values('Date')\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean(); df['Std_Dev'] = df['Close'].rolling(window=20).std()\n",
    "    df['BB_Upper'] = df['SMA_20'] + (df['Std_Dev'] * 2); df['BB_Lower'] = df['SMA_20'] - (df['Std_Dev'] * 2)\n",
    "    delta = df['Close'].diff(1); gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean(); rs = gain / loss\n",
    "    df['RSI_14'] = 100 - (100 / (1 + rs)); high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['Close'].shift()); low_close = np.abs(df['Low'] - df['Close'].shift()); tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['ATR_14'] = tr.rolling(window=14).mean(); df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['SMA_200'] = df['Close'].rolling(window=200).mean()\n",
    "    ema_12 = df['Close'].ewm(span=12, adjust=False).mean(); ema_26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = ema_12 - ema_26; df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean(); return df\n",
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler(); self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.feature_cols = ['SMA_20', 'RSI_14', 'ATR_14', 'MACD']\n",
    "        self.scaled_feature_cols = [f\"{c}_scaled\" for c in self.feature_cols]; self.sector_cols = []\n",
    "    def fit_transform(self, data):\n",
    "        data = data.copy(); sector_encoded = self.encoder.fit_transform(data[['Sector']])\n",
    "        self.sector_cols = [f\"is_{cat}\" for cat in self.encoder.categories_[0]]\n",
    "        sector_df = pd.DataFrame(sector_encoded, index=data.index, columns=self.sector_cols)\n",
    "        scaled_features = self.scaler.fit_transform(data[self.feature_cols])\n",
    "        scaled_df = pd.DataFrame(scaled_features, index=data.index, columns=self.scaled_feature_cols)\n",
    "        processed_df = pd.concat([data[['Date', 'Ticker', 'Close', 'ATR_14']], scaled_df, sector_df], axis=1)\n",
    "        processed_df['Norm_ATR'] = data['ATR_14'] / data.groupby('Ticker')['ATR_14'].transform(lambda x: x.rolling(window=252, min_periods=1).mean())\n",
    "        return processed_df.dropna()\n",
    "    def transform(self, data):\n",
    "        data = data.copy(); sector_encoded = self.encoder.transform(data[['Sector']])\n",
    "        sector_df = pd.DataFrame(sector_encoded, index=data.index, columns=self.sector_cols)\n",
    "        scaled_features = self.scaler.transform(data[self.feature_cols])\n",
    "        scaled_df = pd.DataFrame(scaled_features, index=data.index, columns=self.scaled_feature_cols)\n",
    "        processed_df = pd.concat([data[['Date', 'Ticker', 'Close', 'ATR_14']], scaled_df, sector_df], axis=1)\n",
    "        processed_df['Norm_ATR'] = data['ATR_14'] / data.groupby('Ticker')['ATR_14'].transform(lambda x: x.rolling(window=252, min_periods=1).mean())\n",
    "        return processed_df.dropna()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. SOPHISTICATED LSTM-PPO REINFORCEMENT LEARNING IMPLEMENTATION (Unchanged)\n",
    "# ==============================================================================\n",
    "class StockEnvLSTM:\n",
    "    def __init__(self, data, feature_cols, window_size):\n",
    "        self.data = data.reset_index(drop=True); self.features = self.data[feature_cols].values\n",
    "        self.prices = self.data['Close'].values; self.window_size = window_size\n",
    "        self.current_step = self.window_size; self.done = False; self.holdings = 0\n",
    "        self.initial_cash = 10000; self.cash = self.initial_cash\n",
    "    def _get_state(self):\n",
    "        return self.features[self.current_step - self.window_size : self.current_step]\n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size; self.done = False; self.holdings = 0\n",
    "        self.cash = self.initial_cash; return self._get_state()\n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.current_step]; prev_portfolio_val = self.cash + self.holdings * current_price\n",
    "        if action == 1 and self.cash > 0 and current_price > 0: self.holdings += self.cash / current_price; self.cash = 0\n",
    "        elif action == 0 and self.holdings > 0: self.cash += self.holdings * current_price; self.holdings = 0\n",
    "        self.current_step += 1; self.done = self.current_step >= len(self.data) - 1\n",
    "        new_portfolio_val = self.cash + self.holdings * self.prices[self.current_step]\n",
    "        reward = new_portfolio_val - prev_portfolio_val\n",
    "        next_state = self._get_state() if not self.done else np.zeros_like(self.features[:self.window_size])\n",
    "        return next_state, reward, self.done, {}\n",
    "\n",
    "class ActorCriticLSTM(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, lstm_hidden_dim=128):\n",
    "        super(ActorCriticLSTM, self).__init__(); self.lstm = nn.LSTM(state_dim, lstm_hidden_dim, batch_first=True)\n",
    "        self.layer1 = nn.Linear(lstm_hidden_dim, 128)\n",
    "        self.actor = nn.Linear(128, action_dim); self.critic = nn.Linear(128, 1)\n",
    "    def forward(self, state):\n",
    "        lstm_out, _ = self.lstm(state); x = F.relu(self.layer1(lstm_out[:, -1, :])); return self.actor(x), self.critic(x)\n",
    "\n",
    "def train_ppo_and_generate_trajectories(data, fold_num, writer):\n",
    "    print(\"Starting LSTM-PPO training to generate expert trajectories...\"); all_trajectories = []; trained_models = {}\n",
    "    feature_cols = [c for c in data.columns if c.endswith('_scaled') or c.startswith('is_')]\n",
    "    state_dim = len(feature_cols); action_dim = 2;\n",
    "    for ticker in tqdm(data['Ticker'].unique(), desc=\"Training LSTM-PPO per stock\"):\n",
    "        ticker_data = data[data['Ticker'] == ticker]; env = StockEnvLSTM(ticker_data, feature_cols, CONFIG['WINDOW_SIZE'])\n",
    "        model = ActorCriticLSTM(state_dim, action_dim).to(DEVICE); optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9); gamma = 0.99; gae_lambda = 0.95; policy_clip = 0.2; n_epochs = 4;\n",
    "        for episode in range(25):\n",
    "            state = env.reset(); episode_done = False; episode_rewards = 0\n",
    "            log_probs, values, rewards, states, actions = [], [], [], [], []\n",
    "            while not episode_done:\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(DEVICE); states.append(state)\n",
    "                with torch.no_grad(): action_logits, value = model(state_tensor)\n",
    "                dist = Categorical(logits=action_logits); action = dist.sample(); actions.append(action.item())\n",
    "                log_prob = dist.log_prob(action); log_probs.append(log_prob); values.append(value)\n",
    "                next_state, reward, episode_done, _ = env.step(action.item())\n",
    "                rewards.append(reward); episode_rewards += reward; state = next_state\n",
    "            writer.add_scalar(f'PPO_Fold_{fold_num}/{ticker}/Episode Reward', episode_rewards, episode)\n",
    "            with torch.no_grad(): _, last_value = model(torch.FloatTensor(next_state).unsqueeze(0).to(DEVICE))\n",
    "            returns = torch.zeros(len(rewards) + 1).to(DEVICE); returns[-1] = last_value.squeeze()\n",
    "            for i in reversed(range(len(rewards))): returns[i] = rewards[i] + gamma * returns[i+1]\n",
    "            returns = returns[:-1]\n",
    "            advantages = (returns - torch.cat(values).squeeze()).detach(); advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            states_tensor = torch.FloatTensor(np.array(states)).to(DEVICE); actions_tensor = torch.LongTensor(actions).to(DEVICE); log_probs_tensor = torch.cat(log_probs).detach()\n",
    "            for _ in range(n_epochs):\n",
    "                idxs = np.arange(len(states)); np.random.shuffle(idxs)\n",
    "                for i in range(0, len(states), 512):\n",
    "                    batch_idxs = idxs[i:i+512]; s_b, a_b, lp_b, adv_b, ret_b = states_tensor[batch_idxs], actions_tensor[batch_idxs], log_probs_tensor[batch_idxs], advantages[batch_idxs], returns[batch_idxs]\n",
    "                    new_action_logits, new_values = model(s_b)\n",
    "                    new_dist = Categorical(logits=new_action_logits); new_log_probs = new_dist.log_prob(a_b)\n",
    "                    ratio = torch.exp(new_log_probs - lp_b); surr1 = ratio * adv_b; surr2 = torch.clamp(ratio, 1 - policy_clip, 1 + policy_clip) * adv_b\n",
    "                    actor_loss = -torch.min(surr1, surr2).mean(); critic_loss = F.mse_loss(new_values.squeeze(), ret_b); loss = actor_loss + 0.5 * critic_loss\n",
    "                    optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            scheduler.step()\n",
    "        trained_models[ticker] = model\n",
    "        state = env.reset(); episode_done = False\n",
    "        while not episode_done:\n",
    "            with torch.no_grad(): action_logits, _ = model(torch.FloatTensor(state).unsqueeze(0).to(DEVICE))\n",
    "            action = Categorical(logits=action_logits).sample().item()\n",
    "            next_state, reward, episode_done, _ = env.step(action)\n",
    "            all_trajectories.append({'state': state[-1], 'action': action, 'reward': reward, 'ticker': ticker})\n",
    "            state = next_state\n",
    "    return pd.DataFrame(all_trajectories), trained_models\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. --- NEW --- GAN, WORLD MODEL, AND ADVANCED DT TRAINING\n",
    "# ==============================================================================\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, state_dim, window_size):\n",
    "        super(Generator, self).__init__(); self.window_size, self.state_dim = window_size, state_dim\n",
    "        self.model = nn.Sequential(nn.Linear(latent_dim, 256), nn.ReLU(), nn.Linear(256, 512), nn.ReLU(), nn.Linear(512, window_size * (state_dim + 2)),)\n",
    "    def forward(self, z):\n",
    "        output = self.model(z); return output.view(-1, self.window_size, self.state_dim + 2)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, state_dim, window_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(window_size * (state_dim + 2), 512), nn.LeakyReLU(0.2), nn.Linear(512, 256), nn.LeakyReLU(0.2), nn.Linear(256, 1), nn.Sigmoid(),)\n",
    "    def forward(self, traj):\n",
    "        return self.model(traj.view(traj.size(0), -1))\n",
    "\n",
    "def train_gan_and_synthesize_trajectories(states, actions, rtg, fold_num, writer):\n",
    "    print(\"Training TrajectoryGAN to augment dataset...\")\n",
    "    latent_dim, state_dim, window_size = 100, states.shape[2], states.shape[1]\n",
    "    \n",
    "    # --- CRITICAL FIX --- Correctly shape the rtg tensor for concatenation\n",
    "    real_trajs = torch.cat([\n",
    "        torch.from_numpy(states),\n",
    "        torch.from_numpy(actions).unsqueeze(-1),\n",
    "        torch.from_numpy(rtg).unsqueeze(-1) # Use RTG as the reward signal\n",
    "    ], dim=-1).float()\n",
    "\n",
    "    generator = Generator(latent_dim, state_dim, window_size).to(DEVICE); discriminator = Discriminator(state_dim, window_size).to(DEVICE)\n",
    "    adversarial_loss = nn.BCELoss(); d_optimizer = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999)); g_optimizer = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        for i in range(0, len(real_trajs), 512):\n",
    "            real_batch = real_trajs[i:i+512].to(DEVICE); d_optimizer.zero_grad()\n",
    "            z = torch.randn(real_batch.size(0), latent_dim).to(DEVICE); fake_batch = generator(z)\n",
    "            d_real = discriminator(real_batch); d_fake = discriminator(fake_batch.detach())\n",
    "            d_loss = adversarial_loss(d_real, torch.ones_like(d_real)) + adversarial_loss(d_fake, torch.zeros_like(d_fake))\n",
    "            d_loss.backward(); d_optimizer.step(); g_optimizer.zero_grad()\n",
    "            d_fake_for_g = discriminator(fake_batch); g_loss = adversarial_loss(d_fake_for_g, torch.ones_like(d_fake_for_g))\n",
    "            g_loss.backward(); g_optimizer.step()\n",
    "        writer.add_scalar(f'GAN/Fold_{fold_num}/D_Loss', d_loss.item(), epoch); writer.add_scalar(f'GAN/Fold_{fold_num}/G_Loss', g_loss.item(), epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(len(real_trajs), latent_dim).to(DEVICE)\n",
    "        synthetic_trajs = generator(z).cpu().numpy()\n",
    "    \n",
    "    s_synthetic = synthetic_trajs[:, :, :state_dim]\n",
    "    a_synthetic = np.round(synthetic_trajs[:, :, state_dim:state_dim+1]).astype(int)\n",
    "    r_synthetic = synthetic_trajs[:, :, state_dim+1:]\n",
    "    rtg_synthetic = np.apply_along_axis(lambda x: np.cumsum(x[::-1])[::-1], 1, r_synthetic[:,:,0])\n",
    "\n",
    "    s_combined = np.concatenate([states, s_synthetic]); a_combined = np.concatenate([actions, a_synthetic[:,:,0]])\n",
    "    rtg_combined = np.concatenate([rtg, rtg_synthetic])\n",
    "    # --- FIX --- Correctly create timesteps for the combined dataset\n",
    "    t_combined = np.tile(np.arange(window_size), (len(s_combined), 1))\n",
    "    return s_combined, a_combined, rtg_combined, t_combined\n",
    "\n",
    "class WorldModelLSTM(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__(); self.lstm = nn.LSTM(state_dim + action_dim, 128, batch_first=True, num_layers=2); self.fc = nn.Linear(128, state_dim)\n",
    "    def forward(self, state_seq, action_seq):\n",
    "        action_one_hot = F.one_hot(action_seq.long(), num_classes=2).float(); x = torch.cat([state_seq, action_one_hot], dim=-1)\n",
    "        lstm_out, _ = self.lstm(x); return self.fc(lstm_out)\n",
    "\n",
    "def train_world_model(states, actions, fold_num, writer):\n",
    "    print(\"Training World Model on PPO trajectories...\")\n",
    "    state_dim, action_dim = states.shape[2], 2; world_model = WorldModelLSTM(state_dim, action_dim).to(DEVICE)\n",
    "    optimizer = optim.Adam(world_model.parameters(), lr=1e-3); loss_fn = nn.MSELoss()\n",
    "    input_states = torch.from_numpy(states[:, :-1, :]).float().to(DEVICE)\n",
    "    input_actions = torch.from_numpy(actions[:, :-1]).long().to(DEVICE)\n",
    "    target_states = torch.from_numpy(states[:, 1:, :]).float().to(DEVICE)\n",
    "    for epoch in range(30):\n",
    "        for i in range(0, len(input_states), 1024):\n",
    "            s_b, a_b, s_target_b = input_states[i:i+1024], input_actions[i:i+1024], target_states[i:i+1024]\n",
    "            optimizer.zero_grad(); s_pred_b = world_model(s_b, a_b); loss = loss_fn(s_pred_b, s_target_b)\n",
    "            loss.backward(); optimizer.step()\n",
    "        writer.add_scalar(f'WorldModel/Fold_{fold_num}/Training Loss', loss.item(), epoch)\n",
    "    return world_model\n",
    "\n",
    "def online_finetune_dt(model_dt, train_data, feature_cols, fold_num, writer):\n",
    "    print(\"Starting Online Fine-Tuning for Decision Transformer...\"); optimizer = optim.Adam(model_dt.parameters(), lr=1e-5)\n",
    "    for ticker in tqdm(train_data['Ticker'].unique(), desc=\"Online fine-tuning per stock\"):\n",
    "        ticker_data = train_data[train_data['Ticker'] == ticker]; env = StockEnvLSTM(ticker_data, feature_cols, CONFIG['WINDOW_SIZE'])\n",
    "        state = env.reset(); episode_done = False; rewards, log_probs = [], []\n",
    "        while not episode_done:\n",
    "            context_states_np = np.tile(state[-1], (CONFIG['WINDOW_SIZE'], 1)); context_states = torch.from_numpy(context_states_np).float().unsqueeze(0).to(DEVICE)\n",
    "            context_actions = torch.zeros(1, CONFIG['WINDOW_SIZE'], dtype=torch.long).to(DEVICE); context_rtg = torch.zeros(1, CONFIG['WINDOW_SIZE'], 1, dtype=torch.float32).to(DEVICE)\n",
    "            context_timesteps = torch.arange(CONFIG['WINDOW_SIZE']).reshape(1, CONFIG['WINDOW_SIZE']).to(DEVICE)\n",
    "            action_preds = model_dt(context_states, context_actions, context_rtg, context_timesteps)\n",
    "            dist = Categorical(logits=action_preds[0, -1, :]); action = dist.sample()\n",
    "            next_state, reward, episode_done, _ = env.step(action.item())\n",
    "            log_probs.append(dist.log_prob(action)); rewards.append(reward); state = next_state\n",
    "        returns = []; R = 0\n",
    "        for r in reversed(rewards): R = r + 0.99 * R; returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).to(DEVICE); returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        policy_loss = [-log_prob * R for log_prob, R in zip(log_probs, returns)]\n",
    "        optimizer.zero_grad(); loss = torch.stack(policy_loss).sum(); loss.backward(); optimizer.step()\n",
    "        writer.add_scalar(f'DT_Online/Fold_{fold_num}/{ticker}/Policy Loss', loss.item())\n",
    "    return model_dt\n",
    "\n",
    "class DecisionTransformer(nn.Module): # (Code Unchanged)\n",
    "    def __init__(self, state_dim, act_dim, d_model, n_head, n_layer, max_ep_len):\n",
    "        super().__init__(); self.state_dim, self.act_dim, self.d_model = state_dim, act_dim, d_model\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, n_head, d_model * 4, 0.1, batch_first=True), n_layer)\n",
    "        self.embed_timestep = nn.Embedding(max_ep_len, d_model); self.embed_return = nn.Linear(1, d_model)\n",
    "        self.embed_state = nn.Linear(state_dim, d_model); self.embed_action = nn.Embedding(act_dim, d_model)\n",
    "        self.embed_ln = nn.LayerNorm(d_model); self.predict_action = nn.Sequential(nn.Linear(d_model, act_dim), nn.Softmax(dim=-1))\n",
    "    def forward(self, states, actions, returns_to_go, timesteps):\n",
    "        batch_size, seq_len = states.shape[0], states.shape[1]; state_embeds = self.embed_state(states); action_embeds = self.embed_action(actions)\n",
    "        rtg_embeds = self.embed_return(returns_to_go); time_embeds = self.embed_timestep(timesteps); state_embeds += time_embeds; action_embeds += time_embeds; rtg_embeds += time_embeds\n",
    "        stacked_inputs = torch.stack((rtg_embeds, state_embeds, action_embeds), dim=1).permute(0, 2, 1, 3).reshape(batch_size, 3 * seq_len, self.d_model)\n",
    "        stacked_inputs = self.embed_ln(stacked_inputs); mask = nn.Transformer.generate_square_subsequent_mask(3 * seq_len).to(DEVICE)\n",
    "        transformer_out = self.transformer(stacked_inputs, mask=mask); state_out = transformer_out[:, 1::3]; return self.predict_action(state_out)\n",
    "\n",
    "def train_dt(model, states, actions, rtg, timesteps, fold_num, writer, epochs=5, batch_size=1024): # (Code Unchanged)\n",
    "    model.to(DEVICE).train(); optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    s, a, r, t = torch.from_numpy(states).float(), torch.from_numpy(actions).long(), torch.from_numpy(rtg).float(), torch.from_numpy(timesteps).long()\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(states), batch_size):\n",
    "            s_b, a_b, r_b, t_b = s[i:i+batch_size].to(DEVICE), a[i:i+batch_size].to(DEVICE), r[i:i+batch_size].to(DEVICE), t[i:i+batch_size].to(DEVICE)\n",
    "            if s_b.shape[0] == 0: continue\n",
    "            action_preds = model(s_b, a_b, r_b.unsqueeze(-1), t_b)\n",
    "            loss = F.cross_entropy(action_preds.reshape(-1, model.act_dim), a_b.reshape(-1))\n",
    "            writer.add_scalar(f'DT/Fold_{fold_num}/Initial Training Loss', loss.item(), epoch * (len(states)//batch_size) + (i//batch_size))\n",
    "            optimizer.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n",
    "    return model\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. BACKTESTING (With new MPC backtester)\n",
    "# ==============================================================================\n",
    "def backtest_mpc(world_model, actor_critic_model, data, feature_cols, initial_cash, plan_horizon=10, num_sequences=500):\n",
    "    world_model.eval(); actor_critic_model.eval(); env = StockEnvLSTM(data, feature_cols, CONFIG['WINDOW_SIZE']); state = env.reset(); done = False; portfolio_values = [initial_cash]\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            action_logits, _ = actor_critic_model(state_tensor); dist = Categorical(logits=action_logits)\n",
    "            candidate_actions = dist.sample((num_sequences, plan_horizon)).permute(1, 0)\n",
    "        best_return = -np.inf; best_action = 0; current_state_batch = state_tensor.repeat(num_sequences, 1, 1)\n",
    "        cumulative_rewards = torch.zeros(num_sequences).to(DEVICE)\n",
    "        for t in range(plan_horizon):\n",
    "            actions_at_t = candidate_actions[:, t].unsqueeze(-1)\n",
    "            with torch.no_grad(): next_state_pred_batch = world_model(current_state_batch, actions_at_t.unsqueeze(1))\n",
    "            predicted_rewards = next_state_pred_batch[:, -1, 3] - current_state_batch[:, -1, 3] # Change in scaled MACD\n",
    "            cumulative_rewards += predicted_rewards; current_state_batch = torch.cat([current_state_batch[:, 1:, :], next_state_pred_batch[:, -1, :].unsqueeze(1)], dim=1)\n",
    "        best_sequence_idx = torch.argmax(cumulative_rewards); best_action = candidate_actions[best_sequence_idx, 0].item()\n",
    "        state, _, done, _ = env.step(best_action); current_val = env.cash + env.holdings * env.prices[env.current_step -1]; portfolio_values.append(current_val)\n",
    "    return pd.Series(portfolio_values[1:], index=data.index[CONFIG['WINDOW_SIZE']:])\n",
    "\n",
    "def backtest_dt(model, data, window_size, initial_cash): # (Code Unchanged)\n",
    "    model.eval().to(DEVICE); cash, holdings, portfolio_values, high_water_mark = initial_cash, 0, [initial_cash], initial_cash\n",
    "    feature_cols = [c for c in data.columns if c.endswith('_scaled') or c.startswith('is_')]; state_dim = len(feature_cols)\n",
    "    context_states = torch.zeros(1, window_size, state_dim, device=DEVICE, dtype=torch.float32); context_actions = torch.zeros(1, window_size, dtype=torch.long, device=DEVICE)\n",
    "    context_rtg = torch.zeros(1, window_size, 1, device=DEVICE, dtype=torch.float32); context_timesteps = torch.arange(window_size, device=DEVICE).reshape(1, window_size)\n",
    "    for i in range(len(data)):\n",
    "        current_state_np = data[feature_cols].iloc[i].values; current_state = torch.from_numpy(current_state_np).float().reshape(1, 1, state_dim).to(DEVICE)\n",
    "        context_states = torch.cat([context_states[:, 1:], current_state], dim=1); target_return = initial_cash * 0.15\n",
    "        if CONFIG[\"USE_DYNAMIC_TARGET_RETURN\"] and 'Norm_ATR' in data.columns and not pd.isna(data['Norm_ATR'].iloc[i]) and data['Norm_ATR'].iloc[i] > 0: target_return *= data['Norm_ATR'].iloc[i]\n",
    "        rtg_update = torch.tensor([[[target_return]]], device=DEVICE, dtype=torch.float32); context_rtg = torch.cat([context_rtg[:, 1:], rtg_update], dim=1)\n",
    "        with torch.no_grad(): action_preds = model(context_states, context_actions, context_rtg, context_timesteps)\n",
    "        final_action = torch.argmax(action_preds[0, -1, :]).item()\n",
    "        if CONFIG[\"USE_RISK_MANAGEMENT_OVERLAY\"]:\n",
    "             current_drawdown = (high_water_mark - portfolio_values[-1]) / high_water_mark if high_water_mark > 0 else 0\n",
    "             if current_drawdown > CONFIG[\"MAX_DRAWDOWN_LIMIT\"]: final_action = 0\n",
    "             if 'Norm_ATR' in data.columns and not pd.isna(data['Norm_ATR'].iloc[i]) and data['Norm_ATR'].iloc[i] > CONFIG[\"VOLATILITY_LIMIT_ATR\"]: final_action = 0\n",
    "        current_price = data['Close'].iloc[i]\n",
    "        if final_action == 1 and cash > 0 and current_price > 0: holdings += cash / current_price; cash = 0\n",
    "        elif final_action == 0 and holdings > 0 and current_price > 0: cash += holdings * current_price; holdings = 0\n",
    "        new_value = cash + holdings * current_price; portfolio_values.append(new_value); high_water_mark = max(high_water_mark, new_value)\n",
    "        context_actions = torch.cat([context_actions[:, 1:], torch.tensor([[final_action]], device=DEVICE)], dim=1)\n",
    "    return pd.Series(portfolio_values[1:], index=data.index[:-1])\n",
    "\n",
    "def backtest_ppo(model, data, feature_cols, initial_cash): # (Code Unchanged)\n",
    "    model.to(DEVICE).eval(); env = StockEnvLSTM(data, feature_cols, CONFIG['WINDOW_SIZE']); state = env.reset(); done = False; portfolio_values = [initial_cash]\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action_logits, _ = model(torch.FloatTensor(state).unsqueeze(0).to(DEVICE))\n",
    "            action = Categorical(logits=action_logits).sample().item()\n",
    "        state, _, done, _ = env.step(action); current_val = env.cash + env.holdings * env.prices[env.current_step -1]; portfolio_values.append(current_val)\n",
    "    return pd.Series(portfolio_values[1:], index=data.index[CONFIG['WINDOW_SIZE']:])\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. TRADITIONAL STRATEGIES & REPORTING (Unchanged)\n",
    "# ==============================================================================\n",
    "def backtest_traditional(data, signals, initial_cash): # (Code Unchanged)\n",
    "    cash, holdings, portfolio_values = initial_cash, 0, []\n",
    "    for i in range(len(data)):\n",
    "        signal = signals.iloc[i]; current_price = data['Close'].iloc[i]\n",
    "        if signal == 1 and holdings == 0 and current_price > 0: holdings = cash / current_price; cash = 0\n",
    "        elif signal == -1 and holdings > 0 and current_price > 0: cash = holdings * current_price; holdings = 0\n",
    "        portfolio_values.append(cash + holdings * current_price)\n",
    "    return pd.Series(portfolio_values, index=data.index)\n",
    "def generate_ma_cross_signals(data, short_window=50, long_window=200): # (Code Unchanged)\n",
    "    signals = pd.Series(index=data.index, dtype=int); short_ma = data['Close'].rolling(window=short_window).mean()\n",
    "    long_ma = data['Close'].rolling(window=long_window).mean(); signals[short_ma > long_ma] = 1\n",
    "    signals[short_ma < long_ma] = -1; return signals.ffill().fillna(0)\n",
    "def generate_rsi_signals(data, buy_threshold=30, sell_threshold=70): # (Code Unchanged)\n",
    "    signals = pd.Series(index=data.index, dtype=int); signals[data['RSI_14'] < buy_threshold] = 1\n",
    "    signals[data['RSI_14'] > sell_threshold] = -1; return signals.ffill().fillna(0)\n",
    "def generate_macd_signals(data): # (Code Unchanged)\n",
    "    signals = pd.Series(index=data.index, dtype=int); signals[data['MACD'] > data['MACD_Signal']] = 1\n",
    "    signals[data['MACD'] < data['MACD_Signal']] = -1; return signals.ffill().fillna(0)\n",
    "class ParameterOptimizer: # (Code Unchanged)\n",
    "    def __init__(self, strategy_func, param_grid, backtest_func):\n",
    "        self.strategy_func = strategy_func; self.param_grid = param_grid; self.backtest_func = backtest_func\n",
    "    def find_best_params_per_stock(self, train_data, tickers):\n",
    "        optimal_params_for_all_stocks = {}\n",
    "        keys, values = zip(*self.param_grid.items())\n",
    "        param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "        if 'short_window' in self.param_grid and 'long_window' in self.param_grid: param_combinations = [p for p in param_combinations if p['short_window'] < p['long_window']]\n",
    "        for ticker in tickers:\n",
    "            ticker_data = train_data[train_data['Ticker'] == ticker]\n",
    "            if ticker_data.empty: continue; best_ticker_params, best_ticker_sharpe = {}, -np.inf\n",
    "            for params in param_combinations:\n",
    "                signals = self.strategy_func(ticker_data, **params); portfolio = self.backtest_func(ticker_data, signals, 10000)\n",
    "                returns = portfolio.pct_change().dropna(); sharpe_ratio = 0\n",
    "                if not returns.empty and returns.std() > 0: sharpe_ratio = returns.mean() / returns.std()\n",
    "                if sharpe_ratio > best_ticker_sharpe: best_ticker_sharpe, best_ticker_params = sharpe_ratio, params\n",
    "            optimal_params_for_all_stocks[ticker] = best_ticker_params\n",
    "        print(f\"Completed per-stock optimization for {self.strategy_func.__name__}\"); return optimal_params_for_all_stocks\n",
    "def calculate_performance_metrics(portfolio_values): # (Code Unchanged)\n",
    "    if portfolio_values.empty or portfolio_values.iloc[0] == 0: return None\n",
    "    portfolio_values = portfolio_values[~portfolio_values.index.duplicated(keep='first')]; returns = portfolio_values.pct_change().dropna()\n",
    "    if returns.empty or returns.std() == 0: return None\n",
    "    total_return = (portfolio_values.iloc[-1] - portfolio_values.iloc[0]) / portfolio_values.iloc[0]\n",
    "    num_years = (portfolio_values.index[-1] - portfolio_values.index[0]).days / 365.25 if len(portfolio_values.index) > 1 else 0\n",
    "    annualized_return = (1 + total_return) ** (1/num_years) - 1 if num_years > 0 else total_return\n",
    "    annualized_volatility = returns.std() * np.sqrt(252); sharpe_ratio = annualized_return / annualized_volatility if annualized_volatility > 0 else 0\n",
    "    high_water_mark = portfolio_values.cummax(); drawdown = (portfolio_values - high_water_mark) / high_water_mark\n",
    "    max_drawdown = drawdown.min(); calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown < 0 else 0\n",
    "    return {\"Total Return\": total_return, \"Annualized Return\": annualized_return, \"Annualized Volatility\": annualized_volatility, \"Max Drawdown\": max_drawdown, \"Sharpe Ratio\": sharpe_ratio, \"Calmar Ratio\": calmar_ratio}\n",
    "def print_performance_summary(name, metrics): # (Code Unchanged)\n",
    "    print(f\"\\n--- Performance: {name} ---\"); print(f\"Total Return: {metrics['Total Return']:.2%}\")\n",
    "    print(f\"Annualized Return: {metrics['Annualized Return']:.2%}\"); print(f\"Annualized Volatility: {metrics['Annualized Volatility']:.2%}\")\n",
    "    print(f\"Max Drawdown: {metrics['Max Drawdown']:.2%}\"); print(f\"Sharpe Ratio: {metrics['Sharpe Ratio']:.2f}\"); print(f\"Calmar Ratio: {metrics['Calmar Ratio']:.2f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. MAIN EXECUTION LOGIC\n",
    "# ==============================================================================\n",
    "def launch_tensorboard(): # (Code Unchanged)\n",
    "    log_dir = 'logs'; proc = subprocess.Popen(['tensorboard', '--logdir', log_dir, '--port', '6006'])\n",
    "    print(\"Waiting 5s for TensorBoard to start...\"); time.sleep(5)\n",
    "    url = \"http://localhost:6006/\"; print(f\"Opening TensorBoard at {url}\"); webbrowser.open(url)\n",
    "    return proc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = create_multi_stock_dataframe()\n",
    "    if not CONFIG[\"USE_WALK_FORWARD_VALIDATION\"]: pass\n",
    "    else:\n",
    "        print(\"Starting Main Walk-Forward Validation Process...\")\n",
    "        df['Date'] = pd.to_datetime(df['Date']); df = df.set_index('Date'); df.sort_index(inplace=True)\n",
    "        ticker_to_sector = df[['Ticker', 'Sector']].drop_duplicates().set_index('Ticker')['Sector'].to_dict()\n",
    "        os.makedirs('logs', exist_ok=True); writer = SummaryWriter('logs')\n",
    "        unique_years = df.index.year.unique(); train_window_yrs, test_window_yrs = 8, 2; all_folds_stock_results = []\n",
    "        rsi_optimizer = ParameterOptimizer(generate_rsi_signals, {'buy_threshold': [25, 30, 35], 'sell_threshold': [70, 75, 80]}, backtest_traditional)\n",
    "        ma_optimizer = ParameterOptimizer(generate_ma_cross_signals, {'short_window': [20, 30, 50], 'long_window': [100, 150, 200]}, backtest_traditional)\n",
    "\n",
    "        for i in range(train_window_yrs, len(unique_years), test_window_yrs):\n",
    "            fold_num = i // test_window_yrs; train_start_year, train_end_year = unique_years[i - train_window_yrs], unique_years[i - 1]\n",
    "            test_start_year = unique_years[i]; test_end_year = unique_years[min(i + test_window_yrs - 1, len(unique_years)-1)]\n",
    "            print(f\"\\n===== FOLD {fold_num}: Training on {train_start_year}-{train_end_year}, Testing on {test_start_year}-{test_end_year} =====\")\n",
    "            train_df, test_df = df.loc[str(train_start_year):str(train_end_year)], df.loc[str(test_start_year):str(test_end_year)]\n",
    "            engineer = FeatureEngineer(); processed_train = engineer.fit_transform(train_df.reset_index()); processed_test = engineer.transform(test_df.reset_index())\n",
    "            \n",
    "            ppo_trajectories, trained_ppo_models = train_ppo_and_generate_trajectories(processed_train, fold_num, writer)\n",
    "            s_ppo, a_ppo, rtg_ppo, t_ppo = get_trajectories(ppo_trajectories, CONFIG[\"WINDOW_SIZE\"])\n",
    "            if s_ppo.shape[0] < 1: print(\"Not enough PPO data. Skipping fold.\"); continue\n",
    "            \n",
    "            s_aug, a_aug, rtg_aug, t_aug = train_gan_and_synthesize_trajectories(s_ppo, a_ppo, rtg_ppo, fold_num, writer)\n",
    "            world_model = train_world_model(s_ppo, a_ppo, fold_num, writer)\n",
    "            \n",
    "            state_dim = s_aug.shape[2]; model_dt = DecisionTransformer(state_dim, 2, d_model=128, n_head=4, n_layer=3, max_ep_len=10000)\n",
    "            print(\"Training Decision Transformer on Augmented PPO+GAN trajectories...\"); \n",
    "            model_dt = train_dt(model_dt, s_aug, a_aug, rtg_aug, t_aug, fold_num=fold_num, writer=writer, epochs=5)\n",
    "            model_dt = online_finetune_dt(model_dt, processed_train, engineer.scaled_feature_cols + engineer.sector_cols, fold_num, writer)\n",
    "            \n",
    "            tickers = test_df['Ticker'].unique()\n",
    "            if len(tickers) == 0: continue\n",
    "            cash_per_stock = CONFIG[\"INITIAL_CASH\"] / len(tickers)\n",
    "            print(\"Optimizing traditional strategies for each stock...\")\n",
    "            best_rsi_params_per_stock = rsi_optimizer.find_best_params_per_stock(train_df, train_df['Ticker'].unique())\n",
    "            best_ma_params_per_stock = ma_optimizer.find_best_params_per_stock(train_df, train_df['Ticker'].unique())\n",
    "            fold_stock_results = {ticker: {} for ticker in tickers}\n",
    "            print(\"Backtesting all strategies...\")\n",
    "            for t in tqdm(tickers, desc=\"Backtesting Tickers\"):\n",
    "                ticker_data = test_df[test_df['Ticker'] == t]; proc_data = processed_test[processed_test['Ticker'] == t]\n",
    "                if ticker_data.empty or proc_data.empty: continue\n",
    "                params = best_ma_params_per_stock.get(t, {}); \n",
    "                if params: fold_stock_results[t]['MA Cross (Optimized)'] = backtest_traditional(ticker_data, generate_ma_cross_signals(ticker_data, **params), cash_per_stock)\n",
    "                params = best_rsi_params_per_stock.get(t, {}); \n",
    "                if params: fold_stock_results[t]['RSI (Optimized)'] = backtest_traditional(ticker_data, generate_rsi_signals(ticker_data, **params), cash_per_stock)\n",
    "                fold_stock_results[t]['MACD'] = backtest_traditional(ticker_data, generate_macd_signals(ticker_data), cash_per_stock)\n",
    "                fold_stock_results[t]['Benchmark'] = ticker_data['Close'] * (cash_per_stock / ticker_data['Close'].iloc[0])\n",
    "                ppo_model = trained_ppo_models.get(t)\n",
    "                if ppo_model: \n",
    "                    fold_stock_results[t]['PPO'] = backtest_ppo(ppo_model, proc_data, engineer.scaled_feature_cols + engineer.sector_cols, cash_per_stock)\n",
    "                    fold_stock_results[t]['Model-Based (MPC)'] = backtest_mpc(world_model, ppo_model, proc_data, engineer.scaled_feature_cols + engineer.sector_cols, cash_per_stock)\n",
    "                fold_stock_results[t]['DT (GAN+Online)'] = backtest_dt(model_dt, proc_data, CONFIG[\"WINDOW_SIZE\"], cash_per_stock)\n",
    "            all_folds_stock_results.append(fold_stock_results)\n",
    "\n",
    "        # FINAL AGGREGATION & REPORTING\n",
    "        print(\"\\n\\n\" + \"=\"*60); print(\"===== FINAL WALK-FORWARD PERFORMANCE SUMMARY =====\"); print(\"=\"*60)\n",
    "        final_stock_results = {}; \n",
    "        for fold_res in all_folds_stock_results:\n",
    "            for ticker, strategies in fold_res.items():\n",
    "                if ticker not in final_stock_results: final_stock_results[ticker] = {}\n",
    "                for strat, series in strategies.items():\n",
    "                    if strat not in final_stock_results[ticker]: final_stock_results[ticker][strat] = []; final_stock_results[ticker][strat].append(series)\n",
    "        for ticker, strats in final_stock_results.items():\n",
    "            for strat, series_list in strats.items():\n",
    "                full_series = pd.concat(series_list); final_stock_results[ticker][strat] = full_series[~full_series.index.duplicated(keep='first')]\n",
    "        \n",
    "        strategy_keys = [\"DT (GAN+Online)\", \"PPO\", \"Model-Based (MPC)\", \"MA Cross (Optimized)\", \"RSI (Optimized)\", \"MACD\", \"Benchmark\"]\n",
    "        results_by_strategy = {strat: {} for strat in strategy_keys}\n",
    "        for ticker, strats in final_stock_results.items():\n",
    "            sector = ticker_to_sector.get(ticker)\n",
    "            if not sector: continue\n",
    "            for strat, series in strats.items():\n",
    "                if strat not in results_by_strategy: continue\n",
    "                if sector not in results_by_strategy[strat]: results_by_strategy[strat][sector] = []; results_by_strategy[strat][sector].append(series)\n",
    "        \n",
    "        for strat, sectors in results_by_strategy.items():\n",
    "            strat_name = strat.replace('Benchmark', 'Buy & Hold'); print(\"\\n\" + \"#\"*60); print(f\"## Strategy: {strat_name}\"); print(\"#\"*60)\n",
    "            all_stock_series = [s for sector_stocks in sectors.values() for s in sector_stocks]\n",
    "            if not all_stock_series: print(\"\\nNo trades were made by this strategy.\"); continue\n",
    "            overall_portfolio = pd.concat(all_stock_series, axis=1).sum(axis=1); overall_portfolio = overall_portfolio[~overall_portfolio.index.duplicated(keep='first')]\n",
    "            overall_metrics = calculate_performance_metrics(overall_portfolio)\n",
    "            if overall_metrics: print_performance_summary(\"Overall Portfolio\", overall_metrics)\n",
    "            print(\"\\n--- Per-Sector Performance Breakdown ---\")\n",
    "            for sector, stock_series_list in sorted(sectors.items()):\n",
    "                sector_metrics = [m for s in stock_series_list if (m := calculate_performance_metrics(s)) is not None]\n",
    "                if not sector_metrics: continue\n",
    "                avg_metrics = {k: np.mean([m[k] for m in sector_metrics]) for k in sector_metrics[0]}\n",
    "                min_metrics = {k: np.min([m[k] for m in sector_metrics]) for k in sector_metrics[0]}\n",
    "                max_metrics = {k: np.max([m[k] for m in sector_metrics]) for k in sector_metrics[0]}\n",
    "                print(f\"\\n Sector: {sector} ({len(stock_series_list)} stocks)\"); print(f\" {'Metric':<25} | {'Average':>12} | {'Max':>12} | {'Min':>12}\"); print(\"-\" * 65)\n",
    "                for k in avg_metrics:\n",
    "                    is_percent = \"Return\" in k or \"Volatility\" in k or \"Drawdown\" in k\n",
    "                    avg_str = f\"{avg_metrics[k]:.2%}\" if is_percent else f\"{avg_metrics[k]:.2f}\"\n",
    "                    max_str = f\"{max_metrics[k]:.2%}\" if is_percent else f\"{max_metrics[k]:.2f}\"\n",
    "                    min_str = f\"{min_metrics[k]:.2%}\" if is_percent else f\"{min_metrics[k]:.2f}\"\n",
    "                    print(f\" {k:<25} | {avg_str:>12} | {max_str:>12} | {min_str:>12}\")\n",
    "        writer.close()\n",
    "        launch_tensorboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
