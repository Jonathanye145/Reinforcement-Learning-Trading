{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7455ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonye/anaconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_381549/865292771.py:31: DtypeWarning: Columns (2,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(finance_news)\n",
      "Processing batches (chunk 1):  28%|██▊       | 27/98 [00:26<01:10,  1.01it/s]\n",
      "Processing chunks:   0%|          | 0/7 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 111\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m scores, sentiments\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Apply sentiment analysis\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m scores, sentiments = \u001b[43mbatch_sentiment_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mbatch_sentiment_scores\u001b[39m\u001b[34m(series, tokenizer, batch_size, chunk_size)\u001b[39m\n\u001b[32m     81\u001b[39m inputs = tokenizer(\n\u001b[32m     82\u001b[39m     batch[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     83\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m     padding=\u001b[33m'\u001b[39m\u001b[33mlongest\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     87\u001b[39m ).to(device)\n\u001b[32m     89\u001b[39m outputs = model(input_ids=inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m], attention_mask=inputs[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m probs = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy()\n\u001b[32m     91\u001b[39m batch_scores = np.max(probs, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     92\u001b[39m batch_sentiments = [labels[np.argmax(p)] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m probs]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from typing import Optional, Iterable\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Construct relative path\n",
    "finance_news = os.path.join(\".\", \"FINANCIAL NEWS\", \"finance_data.csv\")\n",
    "\n",
    "# Minimal file path check\n",
    "if not os.path.exists(finance_news):\n",
    "    print(f\"File not found at: {os.path.abspath(finance_news)}\")\n",
    "    exit()\n",
    "\n",
    "# Load datasets\n",
    "df = pd.read_csv(finance_news)\n",
    "sp500_tickers = kagglehub.dataset_load(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"andrewmvd/sp-500-stocks\",\n",
    "    \"sp500_companies.csv\",\n",
    ")\n",
    "\n",
    "# Inner merge at the start\n",
    "df = pd.merge(df, sp500_tickers[['Symbol']], how='inner', left_on='Stock_symbol', right_on='Symbol')\n",
    "df = df.drop(columns=['Symbol'])\n",
    "\n",
    "# Verify and process columns\n",
    "required_columns = ['Date', 'Article_title', 'Stock_symbol', 'Article']\n",
    "if not all(col in df.columns for col in required_columns):\n",
    "    print(\"Error: Missing required columns. Available columns:\", list(df.columns))\n",
    "    exit()\n",
    "\n",
    "# Prepare text data\n",
    "df['text'] = df['Article_title'].astype(str) + ' ' + df['Article'].fillna('').astype(str)\n",
    "df = df[['Date', 'Stock_symbol', 'text']]\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device).eval()\n",
    "\n",
    "# Custom Dataset for lazy tokenization\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, series, max_length=512):\n",
    "        self.series = series\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.series)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.series.iloc[idx]) if not pd.isna(self.series.iloc[idx]) else \"\"\n",
    "        return {'text': text, 'is_empty': text.strip() == \"\"}\n",
    "\n",
    "# Batch inference function with mixed precision\n",
    "def batch_sentiment_scores(series, tokenizer, batch_size=32, chunk_size=100000):\n",
    "    scores, sentiments = [], []\n",
    "    labels = ['positive', 'negative', 'neutral']\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "\n",
    "    for start in tqdm(range(0, len(series), chunk_size), desc=\"Processing chunks\"):\n",
    "        dataset = TextDataset(series[start:min(start + chunk_size, len(series))])\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=f\"Processing batches (chunk {start//chunk_size + 1})\"):\n",
    "                inputs = tokenizer(\n",
    "                    batch['text'],\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=512,\n",
    "                    truncation=True,\n",
    "                    padding='longest'\n",
    "                ).to(device)\n",
    "\n",
    "                outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "                batch_scores = np.max(probs, axis=1)\n",
    "                batch_sentiments = [labels[np.argmax(p)] for p in probs]\n",
    "\n",
    "                # Handle empty texts\n",
    "                for i, empty in enumerate(batch['is_empty']):\n",
    "                    if empty:\n",
    "                        batch_scores[i] = np.nan\n",
    "                        batch_sentiments[i] = \"neutral\"\n",
    "\n",
    "                scores.extend(batch_scores)\n",
    "                sentiments.extend(batch_sentiments)\n",
    "\n",
    "                if is_cuda:\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    return scores, sentiments\n",
    "\n",
    "# Apply sentiment analysis\n",
    "scores, sentiments = batch_sentiment_scores(df['text'], tokenizer, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d129c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign results\n",
    "df['sentiment_score'] = pd.Series(scores, dtype=\"float32\")\n",
    "df['sentiment'] = pd.Series(sentiments, dtype=\"category\")\n",
    "\n",
    "# Process final DataFrame\n",
    "df['sentiment_numeric'] = df['sentiment'].map({'positive': 1, 'neutral': 0, 'negative': -1}).astype('int64')\n",
    "df['adjusted_sentiment_score'] = df['sentiment_numeric'] * df['sentiment_score']\n",
    "df['date_only'] = pd.to_datetime(df['Date']).dt.date\n",
    "\n",
    "# Aggregate data\n",
    "df = df[[\"date_only\", \"Stock_symbol\", \"adjusted_sentiment_score\"]].groupby(\n",
    "    ['Stock_symbol', 'date_only']\n",
    ").agg({'adjusted_sentiment_score': 'mean'}).reset_index()\n",
    "\n",
    "# Save and print results\n",
    "df.to_csv('output.csv', index=False)\n",
    "print(\"Final DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Clean up\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94228143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you already have the DataFrame sp500_ticker with a 'Symbol' column\n",
    "# Example: sp500_ticker = pd.read_csv(\"sp500.csv\")\n",
    "# Make sure symbols are in list form\n",
    "tickers = sp500_tickers['Symbol'].dropna().unique().tolist()\n",
    "\n",
    "# Download historical data for all tickers\n",
    "# 'group_by' keeps data per ticker in separate subframes\n",
    "start_date = df['date_only'].min()\n",
    "end_date = df['date_only'].max() \n",
    "\n",
    "data = yf.download(\n",
    "    tickers=tickers,\n",
    "    start=\"2009-01-01\",\n",
    "    end=\"2020-12-31\",\n",
    "    group_by='ticker',\n",
    "    auto_adjust=False,\n",
    "    threads=True\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex DataFrame\n",
    "sp500_history = data.stack(level=0).reset_index()\n",
    "sp500_history.columns.name = None  # remove any column name metadata\n",
    "\n",
    "# Rename columns for clarity (optional)\n",
    "sp500_history.rename(columns={\n",
    "    'level_1': 'Ticker'\n",
    "}, inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "sp500_history.to_csv(\"sp500_yfinance_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4553a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Iterable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Basic moving averages\n",
    "# ---------------------------\n",
    "def sma(series: cudf.Series, window: int) -> cudf.Series:\n",
    "    return series.rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "def wma(series: cudf.Series, window: int) -> cudf.Series:\n",
    "    weights = cp.arange(1, window + 1)\n",
    "    def wma_calc(prices):\n",
    "        return cp.dot(prices, weights)/weights.sum()\n",
    "    return series.rolling(window).apply(wma_calc)\n",
    "\n",
    "def ema(series: cudf.Series, window: int) -> cudf.Series:\n",
    "    return series.ewm(span=window, adjust=False).mean()\n",
    "\n",
    "# ---------------------------\n",
    "# Momentum / Oscillators\n",
    "# ---------------------------\n",
    "def rsi(series: cudf.Series, window: int = 14) -> cudf.Series:\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    ma_up = up.ewm(alpha=1/window, adjust=False).mean()\n",
    "    ma_down = down.ewm(alpha=1/window, adjust=False).mean()\n",
    "    rs = ma_up / ma_down\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def macd(series: cudf.Series, fast: int = 12, slow: int = 26, signal: int = 9):\n",
    "    fast_ema = ema(series, fast)\n",
    "    slow_ema = ema(series, slow)\n",
    "    macd_line = fast_ema - slow_ema\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    hist = macd_line - signal_line\n",
    "    return macd_line, signal_line, hist\n",
    "\n",
    "def stochastic_oscillator(high: cudf.Series, low: cudf.Series, close: cudf.Series, k_window: int = 14, d_window: int = 3):\n",
    "    lowest_low = low.rolling(window=k_window, min_periods=1).min()\n",
    "    highest_high = high.rolling(window=k_window, min_periods=1).max()\n",
    "    percent_k = 100 * (close - lowest_low) / (highest_high - lowest_low)\n",
    "    percent_d = percent_k.rolling(window=d_window, min_periods=1).mean()\n",
    "    return percent_k, percent_d\n",
    "\n",
    "def williams_r(high: cudf.Series, low: cudf.Series, close: cudf.Series, window: int = 14) -> cudf.Series:\n",
    "    highest_high = high.rolling(window=window, min_periods=1).max()\n",
    "    lowest_low = low.rolling(window=window, min_periods=1).min()\n",
    "    wr = -100 * (highest_high - close) / (highest_high - lowest_low)\n",
    "    return wr\n",
    "\n",
    "def roc(series: cudf.Series, window: int = 12) -> cudf.Series:\n",
    "    return series.pct_change(window) * 100\n",
    "\n",
    "# ---------------------------\n",
    "# Volatility / Bands / Ranges\n",
    "# ---------------------------\n",
    "def bollinger_bands(series: cudf.Series, window: int = 20, n_std: float = 2.0):\n",
    "    mid = sma(series, window)\n",
    "    std = series.rolling(window=window, min_periods=1).std()\n",
    "    upper = mid + n_std * std\n",
    "    lower = mid - n_std * std\n",
    "    bandwidth = (upper - lower) / mid\n",
    "    percent_b = (series - lower) / (upper - lower)\n",
    "    return mid, upper, lower, bandwidth, percent_b\n",
    "\n",
    "def atr(high: cudf.Series, low: cudf.Series, close: cudf.Series, window: int = 14) -> cudf.Series:\n",
    "    high_low = high - low\n",
    "    high_prevclose = (high - close.shift(1)).abs()\n",
    "    low_prevclose = (low - close.shift(1)).abs()\n",
    "    tr = cudf.concat([high_low, high_prevclose, low_prevclose], axis=1).max(axis=1)\n",
    "    return tr.ewm(alpha=1/window, adjust=False).mean()\n",
    "\n",
    "def keltner_channels(high: cudf.Series, low: cudf.Series, close: cudf.Series, ema_window: int = 20, atr_window: int = 10, multiplier: float = 2.0):\n",
    "    mid = ema(close, ema_window)\n",
    "    atr_val = atr(high, low, close, atr_window)\n",
    "    upper = mid + multiplier * atr_val\n",
    "    lower = mid - multiplier * atr_val\n",
    "    return mid, upper, lower\n",
    "\n",
    "def donchian_channel(high: cudf.Series, low: cudf.Series, window: int = 20):\n",
    "    upper = high.rolling(window=window, min_periods=1).max()\n",
    "    lower = low.rolling(window=window, min_periods=1).min()\n",
    "    mid = (upper + lower) / 2\n",
    "    return mid, upper, lower\n",
    "\n",
    "# ---------------------------\n",
    "# Volume-based indicators\n",
    "# ---------------------------\n",
    "def obv(close: cudf.Series, volume: cudf.Series) -> cudf.Series:\n",
    "    direction = cp.sign(close.diff()).fillna(0)\n",
    "    return (direction * volume).fillna(0).cumsum()\n",
    "\n",
    "def chaikin_adi(high: cudf.Series, low: cudf.Series, close: cudf.Series, volume: cudf.Series) -> cudf.Series:\n",
    "    mfm = ((close - low) - (high - close)) / (high - low)\n",
    "    mfm = mfm.replace([cp.inf, -cp.inf], 0).fillna(0)\n",
    "    ad = (mfm * volume).cumsum()\n",
    "    return ad\n",
    "\n",
    "def money_flow_index(high: cudf.Series, low: cudf.Series, close: cudf.Series, volume: cudf.Series, window: int = 14) -> cudf.Series:\n",
    "    tp = (high + low + close) / 3\n",
    "    mf = tp * volume\n",
    "    positive = mf.where(tp > tp.shift(1), 0.0)\n",
    "    negative = mf.where(tp < tp.shift(1), 0.0)\n",
    "    pos_mf = positive.rolling(window=window, min_periods=1).sum()\n",
    "    neg_mf = negative.rolling(window=window, min_periods=1).sum()\n",
    "    mfi = 100 * (pos_mf / (pos_mf + neg_mf))\n",
    "    return mfi\n",
    "\n",
    "def force_index(close: cudf.Series, volume: cudf.Series, window: int = 13) -> cudf.Series:\n",
    "    fi = close.diff() * volume\n",
    "    return fi.ewm(span=window, adjust=False).mean()\n",
    "\n",
    "def vwap(df: cudf.DataFrame, window: Optional[int] = None) -> cudf.Series:\n",
    "    tp = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    pv = tp * df['Volume']\n",
    "    if window is None:\n",
    "        return pv.cumsum() / df['Volume'].cumsum()\n",
    "    else:\n",
    "        return pv.rolling(window).sum() / df['Volume'].rolling(window).sum()\n",
    "\n",
    "# ---------------------------\n",
    "# Trend / Directional Movement (ADX)\n",
    "# ---------------------------\n",
    "def _dm_plus(high: cudf.Series, low: cudf.Series) -> cudf.Series:\n",
    "    up_move = high.diff()\n",
    "    down_move = -low.diff()\n",
    "    dm_plus = up_move.where((up_move > down_move) & (up_move > 0), 0.0)\n",
    "    return dm_plus\n",
    "\n",
    "def _dm_minus(high: cudf.Series, low: cudf.Series) -> cudf.Series:\n",
    "    up_move = high.diff()\n",
    "    down_move = -low.diff()\n",
    "    dm_minus = down_move.where((down_move > up_move) & (down_move > 0), 0.0)\n",
    "    return dm_minus\n",
    "\n",
    "def adx(high: cudf.Series, low: cudf.Series, close: cudf.Series, window: int = 14) -> cudf.Series:\n",
    "    tr = cudf.concat([\n",
    "        (high - low).abs(),\n",
    "        (high - close.shift(1)).abs(),\n",
    "        (low - close.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    atr_ = tr.ewm(alpha=1/window, adjust=False).mean()\n",
    "    dm_p = _dm_plus(high, low).ewm(alpha=1/window, adjust=False).mean()\n",
    "    dm_m = _dm_minus(high, low).ewm(alpha=1/window, adjust=False).mean()\n",
    "    di_p = 100 * (dm_p / atr_)\n",
    "    di_m = 100 * (dm_m / atr_)\n",
    "    dx = 100 * (di_p - di_m).abs() / (di_p + di_m)\n",
    "    adx_series = dx.ewm(alpha=1/window, adjust=False).mean()\n",
    "    return adx_series\n",
    "\n",
    "# ---------------------------\n",
    "# Statistical / Composite\n",
    "# ---------------------------\n",
    "def cci(high: cudf.Series, low: cudf.Series, close: cudf.Series, window: int = 20) -> cudf.Series:\n",
    "    tp = (high + low + close) / 3\n",
    "    ma = tp.rolling(window=window, min_periods=1).mean()\n",
    "    mad = tp.rolling(window=window, min_periods=1).apply(lambda x: cp.fabs(x - x.mean()).mean())\n",
    "    cci_val = (tp - ma) / (0.015 * mad)\n",
    "    return cci_val\n",
    "\n",
    "# ---------------------------\n",
    "# Parabolic SAR (loop implementation, CPU fallback)\n",
    "# ---------------------------\n",
    "def parabolic_sar(high: cudf.Series, low: cudf.Series, close: cudf.Series, af_start: float = 0.02, af_step: float = 0.02, af_max: float = 0.2) -> cudf.Series:\n",
    "    # Convert to pandas for iterative calculation\n",
    "    high_pd = high.to_pandas()\n",
    "    low_pd = low.to_pandas()\n",
    "    close_pd = close.to_pandas()\n",
    "    highs = high_pd.values\n",
    "    lows = low_pd.values\n",
    "    length = len(highs)\n",
    "    if length == 0:\n",
    "        return cudf.Series(dtype=float)\n",
    "\n",
    "    sar = np.zeros(length)\n",
    "    bull = True\n",
    "    af = af_start\n",
    "    ep = highs[0]\n",
    "    sar[0] = lows[0]\n",
    "\n",
    "    for i in range(1, length):\n",
    "        prev = sar[i - 1]\n",
    "        if bull:\n",
    "            sar[i] = prev + af * (ep - prev)\n",
    "            sar[i] = min(sar[i], lows[i-1], lows[i-2] if i >= 2 else lows[i-1])\n",
    "            if lows[i] < sar[i]:\n",
    "                bull = False\n",
    "                sar[i] = ep\n",
    "                ep = lows[i]\n",
    "                af = af_start\n",
    "        else:\n",
    "            sar[i] = prev + af * (ep - prev)\n",
    "            sar[i] = max(sar[i], highs[i-1], highs[i-2] if i >= 2 else highs[i-1])\n",
    "            if highs[i] > sar[i]:\n",
    "                bull = True\n",
    "                sar[i] = ep\n",
    "                ep = highs[i]\n",
    "                af = af_start\n",
    "        if bull:\n",
    "            if highs[i] > ep:\n",
    "                ep = highs[i]\n",
    "                af = min(af + af_step, af_max)\n",
    "        else:\n",
    "            if lows[i] < ep:\n",
    "                ep = lows[i]\n",
    "                af = min(af + af_step, af_max)\n",
    "\n",
    "    return cudf.Series(sar, index=high.index)\n",
    "\n",
    "# ---------------------------\n",
    "# Ichimoku Cloud\n",
    "# ---------------------------\n",
    "def ichimoku(high: cudf.Series, low: cudf.Series, close: cudf.Series,\n",
    "             tenkan: int = 9, kijun: int = 26, senkou_b: int = 52, shift: int = 26):\n",
    "    conv = (high.rolling(window=tenkan).max() + low.rolling(window=tenkan).min()) / 2\n",
    "    base = (high.rolling(window=kijun).max() + low.rolling(window=kijun).min()) / 2\n",
    "    span_a = ((conv + base) / 2).shift(shift)\n",
    "    span_b = ((high.rolling(window=senkou_b).max() + low.rolling(window=senkou_b).min()) / 2).shift(shift)\n",
    "    lagging = close.shift(-shift)\n",
    "    return conv, base, span_a, span_b, lagging\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: add indicators for all tickers\n",
    "# ---------------------------\n",
    "def add_all_indicators(df: cudf.DataFrame, additional: Optional[Iterable[str]] = None, prefix: str = \"\") -> cudf.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute and append indicators for all tickers in df, grouped by Ticker.\n",
    "    Ensures date sorting and handles holiday gaps by processing each ticker independently.\n",
    "    \"\"\"\n",
    "    # Validate required columns\n",
    "    required = {\"Ticker\", \"Date\", \"High\", \"Low\", \"Close\", \"Volume\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {required}\")\n",
    "\n",
    "    # Ensure Date is datetime and sort by Ticker and Date\n",
    "    df = df.copy()\n",
    "    df['Date'] = cudf.to_datetime(df['Date'])\n",
    "    df = df.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "    # Define function to apply indicators to a single ticker's data\n",
    "    def apply_indicators(group: cudf.DataFrame) -> cudf.DataFrame:\n",
    "        high = group[\"High\"]\n",
    "        low = group[\"Low\"]\n",
    "        close = group[\"Close\"]\n",
    "        volume = group[\"Volume\"]\n",
    "\n",
    "        # Moving averages\n",
    "        group[f\"{prefix}SMA_20\"] = sma(close, 20)\n",
    "        group[f\"{prefix}SMA_50\"] = sma(close, 50)\n",
    "        group[f\"{prefix}SMA_200\"] = sma(close, 200)\n",
    "        group[f\"{prefix}EMA_12\"] = ema(close, 12)\n",
    "        group[f\"{prefix}EMA_26\"] = ema(close, 26)\n",
    "        group[f\"{prefix}WMA_50\"] = wma(close, 50)\n",
    "\n",
    "        # Momentum\n",
    "        group[f\"{prefix}RSI_14\"] = rsi(close, 14)\n",
    "        macd_line, macd_signal, macd_hist = macd(close)\n",
    "        group[f\"{prefix}MACD\"] = macd_line\n",
    "        group[f\"{prefix}MACD_Signal\"] = macd_signal\n",
    "        group[f\"{prefix}MACD_Hist\"] = macd_hist\n",
    "        stoch_k, stoch_d = stochastic_oscillator(high, low, close)\n",
    "        group[f\"{prefix}Stoch_%K\"] = stoch_k\n",
    "        group[f\"{prefix}Stoch_%D\"] = stoch_d\n",
    "        group[f\"{prefix}WilliamsR_14\"] = williams_r(high, low, close, 14)\n",
    "        group[f\"{prefix}ROC_12\"] = roc(close, 12)\n",
    "\n",
    "        # Volatility/bands\n",
    "        bb_mid, bb_up, bb_low, bb_bw, bb_pctb = bollinger_bands(close, 20, 2)\n",
    "        group[f\"{prefix}BB_Mid\"] = bb_mid\n",
    "        group[f\"{prefix}BB_Upper\"] = bb_up\n",
    "        group[f\"{prefix}BB_Lower\"] = bb_low\n",
    "        group[f\"{prefix}BB_Bandwidth\"] = bb_bw\n",
    "        group[f\"{prefix}BB_pctB\"] = bb_pctb\n",
    "        group[f\"{prefix}ATR_14\"] = atr(high, low, close, 14)\n",
    "        k_mid, k_up, k_low = keltner_channels(high, low, close, 20, 10, 2.0)\n",
    "        group[f\"{prefix}KC_Mid\"] = k_mid\n",
    "        group[f\"{prefix}KC_Upper\"] = k_up\n",
    "        group[f\"{prefix}KC_Lower\"] = k_low\n",
    "        d_mid, d_up, d_low = donchian_channel(high, low, 20)\n",
    "        group[f\"{prefix}Donchian_Mid\"] = d_mid\n",
    "        group[f\"{prefix}Donchian_Upper\"] = d_up\n",
    "        group[f\"{prefix}Donchian_Lower\"] = d_low\n",
    "\n",
    "        # Volume-based\n",
    "        group[f\"{prefix}OBV\"] = obv(close, volume)\n",
    "        group[f\"{prefix}Chaikin_AD\"] = chaikin_adi(high, low, close, volume)\n",
    "        group[f\"{prefix}MFI_14\"] = money_flow_index(high, low, close, volume, 14)\n",
    "        group[f\"{prefix}ForceIndex_13\"] = force_index(close, volume, 13)\n",
    "        group[f\"{prefix}VWAP\"] = vwap(group)\n",
    "\n",
    "        # Trend / Directional\n",
    "        group[f\"{prefix}ADX_14\"] = adx(high, low, close, 14)\n",
    "        group[f\"{prefix}CCI_20\"] = cci(high, low, close, 20)\n",
    "\n",
    "        # Parabolic SAR\n",
    "        try:\n",
    "            group[f\"{prefix}Parabolic_SAR\"] = parabolic_sar(high, low, close)\n",
    "        except Exception:\n",
    "            group[f\"{prefix}Parabolic_SAR\"] = cp.nan\n",
    "\n",
    "        # Ichimoku\n",
    "        conv, base, span_a, span_b, lag = ichimoku(high, low, close)\n",
    "        group[f\"{prefix}Ichimoku_Conv\"] = conv\n",
    "        group[f\"{prefix}Ichimoku_Base\"] = base\n",
    "        group[f\"{prefix}Ichimoku_SpanA\"] = span_a\n",
    "        group[f\"{prefix}Ichimoku_SpanB\"] = span_b\n",
    "        group[f\"{prefix}Ichimoku_Lagging\"] = lag\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Apply indicators to each ticker group\n",
    "    result = df.groupby('Ticker').apply(apply_indicators)\n",
    "\n",
    "    # Clean up infinite values\n",
    "    result = result.replace([cp.inf, -cp.inf], cp.nan)\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------------------------\n",
    "# Plotting helper\n",
    "# ---------------------------\n",
    "def plot_basic_with_indicators(df: cudf.DataFrame, ticker: str):\n",
    "    \"\"\"Plot price + SMA + Bollinger + RSI + MACD for a specific ticker.\"\"\"\n",
    "    # Filter for the specific ticker and convert to pandas for plotting\n",
    "    df_ticker = df[df['Ticker'] == ticker].to_pandas()\n",
    "    df_ticker.set_index('Date', inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    ax1 = plt.subplot(3, 1, 1)\n",
    "    ax1.plot(df_ticker.index, df_ticker[\"Close\"], label=\"Close\")\n",
    "    for col in [\"SMA_20\", \"SMA_50\", \"EMA_12\"]:\n",
    "        if col in df_ticker.columns:\n",
    "            ax1.plot(df_ticker.index, df_ticker[col], label=col)\n",
    "    if \"BB_Upper\" in df_ticker.columns:\n",
    "        ax1.plot(df_ticker.index, df_ticker[\"BB_Upper\"], linestyle=\"--\", label=\"BB Upper\")\n",
    "        ax1.plot(df_ticker.index, df_ticker[\"BB_Lower\"], linestyle=\"--\", label=\"BB Lower\")\n",
    "    ax1.set_title(f\"{ticker} Price & Moving Averages\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = plt.subplot(3, 1, 2, sharex=ax1)\n",
    "    if \"RSI_14\" in df_ticker.columns:\n",
    "        ax2.plot(df_ticker.index, df_ticker[\"RSI_14\"], label=\"RSI_14\")\n",
    "        ax2.axhline(70, linestyle=\"--\", alpha=0.6)\n",
    "        ax2.axhline(30, linestyle=\"--\", alpha=0.6)\n",
    "        ax2.set_ylabel(\"RSI\")\n",
    "        ax2.legend()\n",
    "\n",
    "    ax3 = plt.subplot(3, 1, 3, sharex=ax1)\n",
    "    if \"MACD\" in df_ticker.columns:\n",
    "        ax3.plot(df_ticker.index, df_ticker[\"MACD\"], label=\"MACD\")\n",
    "        ax3.plot(df_ticker.index, df_ticker[\"MACD_Signal\"], label=\"Signal\")\n",
    "        if \"MACD_Hist\" in df_ticker.columns:\n",
    "            ax3.bar(df_ticker.index, df_ticker[\"MACD_Hist\"], label=\"Hist\", alpha=0.5)\n",
    "        ax3.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data creation (replace with actual data loading)\n",
    "    dates = pd.date_range(start='2020-01-01', end='2025-08-07', freq='B')  # Business days to handle holidays\n",
    "    tickers = ['AAPL', 'MSFT']\n",
    "    data = []\n",
    "    for ticker in tickers:\n",
    "        for date in dates:\n",
    "            data.append({\n",
    "                'Ticker': ticker,\n",
    "                'Date': date,\n",
    "                'High': np.random.uniform(100, 200),\n",
    "                'Low': np.random.uniform(80, 180),\n",
    "                'Close': np.random.uniform(90, 190),\n",
    "                'Volume': np.random.randint(1000, 100000)\n",
    "            })\n",
    "    df = cudf.DataFrame(data)\n",
    "\n",
    "    print(\"Computing indicators for all tickers...\")\n",
    "    result_df = add_all_indicators(df)\n",
    "    cols = [c for c in result_df.columns if any(k in c for k in [\"Close\", \"SMA_20\", \"RSI_14\", \"MACD\", \"BB_Upper\", \"ATR_14\", \"ADX_14\", \"VWAP\"])]\n",
    "    print(result_df[cols].tail(5))\n",
    "    plot_basic_with_indicators(result_df, \"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Convert to cuDF DataFrame\n",
    "    sp500_history = cudf.from_pandas(sp500_history)\n",
    "\n",
    "    print(\"Computing indicators for all tickers...\")\n",
    "    result_df = add_all_indicators(sp500_history)\n",
    "    cols = [c for c in result_df.columns if any(k in c for k in [\"Close\", \"SMA_20\", \"RSI_14\", \"MACD\", \"BB_Upper\", \"ATR_14\", \"ADX_14\", \"VWAP\"])]\n",
    "    print(result_df[cols].tail(5))\n",
    "    plot_basic_with_indicators(result_df, \"AAPL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
